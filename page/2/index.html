<!doctype html><html lang=en dir=auto><head><meta name=generator content="Hugo 0.141.0"><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>johnpoint's blog</title>
<meta name=keywords content="blog,johnpoint"><meta name=description content="johnpoint's blog"><meta name=author content="johnpoint"><link rel=canonical href=https://blog.lvcshu.com/><link crossorigin=anonymous href=/assets/css/stylesheet.9a07b9c81cbc95fa741d61ca6489dcdb8491591c19dca31d14a124a4ba95ec80.css integrity="sha256-mge5yBy8lfp0HWHKZInc24SRWRwZ3KMdFKEkpLqV7IA=" rel="preload stylesheet" as=style><link rel=icon href=https://blog.lvcshu.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://blog.lvcshu.com/favicon.ico><link rel=icon type=image/png sizes=32x32 href=https://blog.lvcshu.com/favicon.ico><link rel=apple-touch-icon href=https://blog.lvcshu.com/favicon.ico><link rel=mask-icon href=https://blog.lvcshu.com/favicon.ico><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://blog.lvcshu.com/index.xml><link rel=alternate type=application/json href=https://blog.lvcshu.com/index.json><link rel=alternate hreflang=en href=https://blog.lvcshu.com/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="johnpoint's blog"><meta property="og:description" content="johnpoint's blog"><meta property="og:type" content="website"><meta property="og:url" content="https://blog.lvcshu.com/"><meta property="og:site_name" content="johnpoint's blog"><meta name=twitter:card content="summary"><meta name=twitter:title content="johnpoint's blog"><meta name=twitter:description content="johnpoint's blog"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"johnpoint's blog","url":"https://blog.lvcshu.com/","description":"johnpoint's blog","thumbnailUrl":"https://blog.lvcshu.com/favicon.ico","sameAs":["https://t.me/johnpoint","https://blog.lvcshu.com/index.xml","https://github.com/johnpoint","mailto:me@lvcshu.com"]}</script></head><body class="list dark" id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://blog.lvcshu.com/ accesskey=h title="johnpoint's blog (Alt + H)">johnpoint's blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li></li></ul></div></div><ul id=menu><li><a href=https://blog.lvcshu.com/posts/ title=归档><span>归档</span></a></li><li><a href=https://blog.lvcshu.com/friends/ title=友链><span>友链</span></a></li><li><a href=https://blog.lvcshu.com/about/ title=关于我><span>关于我</span></a></li><li><a href=https://blog.lvcshu.com/search/ title="搜索 (Alt + /)" accesskey=/><span>搜索</span></a></li></ul></nav></header><main class=main><article class=post-entry><header class=entry-header><h2>使用 docker-compose 搭建 clickhouse 集群</h2></header><div class=entry-content><p>Docker Compose 配置 version: '3' services: clickhouse-server-ck1: restart: on-failure:10 # 退出非0重启，尝试10次 image: yandex/clickhouse-server container_name: ck1 networks: - ck-network ports: - "8124:8123" - "9001:9000" - "9010:9004" volumes: - `pwd`/clickhouse/:/var/lib/clickhouse/ - `pwd`/clickhouse-server/:/etc/clickhouse-server/ - `pwd`/log/clickhouse-server/:/var/log/clickhouse-server/ ulimits: nofile: soft: "262144" hard: "262144" depends_on: - zookeeper-1 clickhouse-server-ck2: restart: on-failure:10 # 退出非0重启，尝试10次 image: yandex/clickhouse-server container_name: ck2 networks: - ck-network ports: - "8125:8123" - "9002:9000" - "9011:9004" volumes: - `pwd`/clickhouse2/:/var/lib/clickhouse/ - `pwd`/clickhouse-server2/:/etc/clickhouse-server/ - `pwd`/log/clickhouse-server2/:/var/log/clickhouse-server/ ulimits: nofile: soft: "262144" hard: "262144" depends_on: - zookeeper-1 clickhouse-server-ck3: restart: on-failure:10 # 退出非0重启，尝试10次 image: yandex/clickhouse-server container_name: ck3 networks: - ck-network ports: - "8126:8123" - "9003:9000" - "9012:9004" volumes: - `pwd`/clickhouse3/:/var/lib/clickhouse/ - `pwd`/clickhouse-server3/:/etc/clickhouse-server/ - `pwd`/log/clickhouse-server3/:/var/log/clickhouse-server/ ulimits: nofile: soft: "262144" hard: "262144" depends_on: - zookeeper-1 zookeeper-1: restart: on-failure:10 # 退出非0重启，尝试10次 image: zookeeper:3.8.0 container_name: zookeeper1 networks: - ck-network ports: - "2181:2181" volumes: - `pwd`/zookeeper/conf/:/apache-zookeeper-3.8.0-bin/conf/ - `pwd`/zookeeper/data/:/data - `pwd`/zookeeper/datalog/:/datalog - `pwd`/zookeeper/logs/:/logs ulimits: nofile: soft: "262144" hard: "262144" networks: ck-network: Clickhouse 配置文件 &lt;?xml version="1.0"?> &lt;!-- NOTE: User and query level settings are set up in "users.xml" file. If you have accidentally specified user-level settings here, server won't start. You can either move the settings to the right place inside "users.xml" file or add &lt;skip_check_for_incorrect_settings>1&lt;/skip_check_for_incorrect_settings> here. --> &lt;clickhouse> &lt;logger> &lt;!-- Possible levels [1]: - none (turns off logging) - fatal - critical - error - warning - notice - information - debug - trace - test (not for production usage) [1]: https://github.com/pocoproject/poco/blob/poco-1.9.4-release/Foundation/include/Poco/Logger.h#L105-L114 --> &lt;level>trace&lt;/level> &lt;log>/var/log/clickhouse-server/clickhouse-server.log&lt;/log> &lt;errorlog>/var/log/clickhouse-server/clickhouse-server.err.log&lt;/errorlog> &lt;!-- Rotation policy See https://github.com/pocoproject/poco/blob/poco-1.9.4-release/Foundation/include/Poco/FileChannel.h#L54-L85 --> &lt;size>1000M&lt;/size> &lt;count>10&lt;/count> &lt;!-- &lt;console>1&lt;/console> --> &lt;!-- Default behavior is autodetection (log to console if not daemon mode and is tty) --> &lt;!-- Per level overrides (legacy): For example to suppress logging of the ConfigReloader you can use: NOTE: levels.logger is reserved, see below. --> &lt;!-- &lt;levels> &lt;ConfigReloader>none&lt;/ConfigReloader> &lt;/levels> --> &lt;!-- Per level overrides: For example to suppress logging of the RBAC for default user you can use: (But please note that the logger name maybe changed from version to version, even after minor upgrade) --> &lt;!-- &lt;levels> &lt;logger> &lt;name>ContextAccess (default)&lt;/name> &lt;level>none&lt;/level> &lt;/logger> &lt;logger> &lt;name>DatabaseOrdinary (test)&lt;/name> &lt;level>none&lt;/level> &lt;/logger> &lt;/levels> --> &lt;/logger> &lt;!-- Add headers to response in options request. OPTIONS method is used in CORS preflight requests. --> &lt;!-- It is off by default. Next headers are obligate for CORS.--> &lt;!-- http_options_response> &lt;header> &lt;name>Access-Control-Allow-Origin&lt;/name> &lt;value>*&lt;/value> &lt;/header> &lt;header> &lt;name>Access-Control-Allow-Headers&lt;/name> &lt;value>origin, x-requested-with&lt;/value> &lt;/header> &lt;header> &lt;name>Access-Control-Allow-Methods&lt;/name> &lt;value>POST, GET, OPTIONS&lt;/value> &lt;/header> &lt;header> &lt;name>Access-Control-Max-Age&lt;/name> &lt;value>86400&lt;/value> &lt;/header> &lt;/http_options_response --> &lt;!-- It is the name that will be shown in the clickhouse-client. By default, anything with "production" will be highlighted in red in query prompt. --> &lt;!--display_name>production&lt;/display_name--> &lt;!-- Port for HTTP API. See also 'https_port' for secure connections. This interface is also used by ODBC and JDBC drivers (DataGrip, Dbeaver, ...) and by most of web interfaces (embedded UI, Grafana, Redash, ...). --> &lt;http_port>8123&lt;/http_port> &lt;!-- Port for interaction by native protocol with: - clickhouse-client and other native ClickHouse tools (clickhouse-benchmark, clickhouse-copier); - clickhouse-server with other clickhouse-servers for distributed query processing; - ClickHouse drivers and applications supporting native protocol (this protocol is also informally called as "the TCP protocol"); See also 'tcp_port_secure' for secure connections. --> &lt;tcp_port>9000&lt;/tcp_port> &lt;!-- Compatibility with MySQL protocol. ClickHouse will pretend to be MySQL for applications connecting to this port. --> &lt;mysql_port>9004&lt;/mysql_port> &lt;!-- Compatibility with PostgreSQL protocol. ClickHouse will pretend to be PostgreSQL for applications connecting to this port. --> &lt;postgresql_port>9005&lt;/postgresql_port> &lt;!-- HTTP API with TLS (HTTPS). You have to configure certificate to enable this interface. See the openSSL section below. --> &lt;!-- &lt;https_port>8443&lt;/https_port> --> &lt;!-- Native interface with TLS. You have to configure certificate to enable this interface. See the openSSL section below. --> &lt;!-- &lt;tcp_port_secure>9440&lt;/tcp_port_secure> --> &lt;!-- Native interface wrapped with PROXYv1 protocol PROXYv1 header sent for every connection. ClickHouse will extract information about proxy-forwarded client address from the header. --> &lt;!-- &lt;tcp_with_proxy_port>9011&lt;/tcp_with_proxy_port> --> &lt;!-- Port for communication between replicas. Used for data exchange. It provides low-level data access between servers. This port should not be accessible from untrusted networks. See also 'interserver_http_credentials'. Data transferred over connections to this port should not go through untrusted networks. See also 'interserver_https_port'. --> &lt;interserver_http_port>9009&lt;/interserver_http_port> &lt;!-- Port for communication between replicas with TLS. You have to configure certificate to enable this interface. See the openSSL section below. See also 'interserver_http_credentials'. --> &lt;!-- &lt;interserver_https_port>9010&lt;/interserver_https_port> --> &lt;!-- Hostname that is used by other replicas to request this server. If not specified, than it is determined analogous to 'hostname -f' command. This setting could be used to switch replication to another network interface (the server may be connected to multiple networks via multiple addresses) --> &lt;interserver_http_host>0.0.0.0&lt;/interserver_http_host> &lt;!-- You can specify credentials for authenthication between replicas. This is required when interserver_https_port is accessible from untrusted networks, and also recommended to avoid SSRF attacks from possibly compromised services in your network. --> &lt;!--&lt;interserver_http_credentials> &lt;user>interserver&lt;/user> &lt;password>&lt;/password> &lt;/interserver_http_credentials>--> &lt;!-- Listen specified address. Use :: (wildcard IPv6 address), if you want to accept connections both with IPv4 and IPv6 from everywhere. Notes: If you open connections from wildcard address, make sure that at least one of the following measures applied: - server is protected by firewall and not accessible from untrusted networks; - all users are restricted to subset of network addresses (see users.xml); - all users have strong passwords, only secure (TLS) interfaces are accessible, or connections are only made via TLS interfaces. - users without password have readonly access. See also: https://www.shodan.io/search?query=clickhouse --> &lt;!-- &lt;listen_host>::&lt;/listen_host> --> &lt;!-- Same for hosts without support for IPv6: --> &lt;listen_host>0.0.0.0&lt;/listen_host> &lt;!-- Default values - try listen localhost on IPv4 and IPv6. --> &lt;!-- &lt;listen_host>::1&lt;/listen_host> &lt;listen_host>127.0.0.1&lt;/listen_host> --> &lt;!-- Don't exit if IPv6 or IPv4 networks are unavailable while trying to listen. --> &lt;!-- &lt;listen_try>0&lt;/listen_try> --> &lt;!-- Allow multiple servers to listen on the same address:port. This is not recommended. --> &lt;!-- &lt;listen_reuse_port>0&lt;/listen_reuse_port> --> &lt;!-- &lt;listen_backlog>4096&lt;/listen_backlog> --> &lt;max_connections>4096&lt;/max_connections> &lt;!-- For 'Connection: keep-alive' in HTTP 1.1 --> &lt;keep_alive_timeout>3&lt;/keep_alive_timeout> &lt;!-- gRPC protocol (see src/Server/grpc_protos/clickhouse_grpc.proto for the API) --> &lt;!-- &lt;grpc_port>9100&lt;/grpc_port> --> &lt;grpc> &lt;enable_ssl>false&lt;/enable_ssl> &lt;!-- The following two files are used only if enable_ssl=1 --> &lt;ssl_cert_file>/path/to/ssl_cert_file&lt;/ssl_cert_file> &lt;ssl_key_file>/path/to/ssl_key_file&lt;/ssl_key_file> &lt;!-- Whether server will request client for a certificate --> &lt;ssl_require_client_auth>false&lt;/ssl_require_client_auth> &lt;!-- The following file is used only if ssl_require_client_auth=1 --> &lt;ssl_ca_cert_file>/path/to/ssl_ca_cert_file&lt;/ssl_ca_cert_file> &lt;!-- Default compression algorithm (applied if client doesn't specify another algorithm, see result_compression in QueryInfo). Supported algorithms: none, deflate, gzip, stream_gzip --> &lt;compression>deflate&lt;/compression> &lt;!-- Default compression level (applied if client doesn't specify another level, see result_compression in QueryInfo). Supported levels: none, low, medium, high --> &lt;compression_level>medium&lt;/compression_level> &lt;!-- Send/receive message size limits in bytes. -1 means unlimited --> &lt;max_send_message_size>-1&lt;/max_send_message_size> &lt;max_receive_message_size>-1&lt;/max_receive_message_size> &lt;!-- Enable if you want very detailed logs --> &lt;verbose_logs>false&lt;/verbose_logs> &lt;/grpc> &lt;!-- Used with https_port and tcp_port_secure. Full ssl options list: https://github.com/ClickHouse-Extras/poco/blob/master/NetSSL_OpenSSL/include/Poco/Net/SSLManager.h#L71 --> &lt;openSSL> &lt;server> &lt;!-- Used for https server AND secure tcp port --> &lt;!-- openssl req -subj "/CN=localhost" -new -newkey rsa:2048 -days 365 -nodes -x509 -keyout /etc/clickhouse-server/server.key -out /etc/clickhouse-server/server.crt --> &lt;certificateFile>/etc/clickhouse-server/server.crt&lt;/certificateFile> &lt;privateKeyFile>/etc/clickhouse-server/server.key&lt;/privateKeyFile> &lt;!-- dhparams are optional. You can delete the &lt;dhParamsFile> element. To generate dhparams, use the following command: openssl dhparam -out /etc/clickhouse-server/dhparam.pem 4096 Only file format with BEGIN DH PARAMETERS is supported. --> &lt;dhParamsFile>/etc/clickhouse-server/dhparam.pem&lt;/dhParamsFile> &lt;verificationMode>none&lt;/verificationMode> &lt;loadDefaultCAFile>true&lt;/loadDefaultCAFile> &lt;cacheSessions>true&lt;/cacheSessions> &lt;disableProtocols>sslv2,sslv3&lt;/disableProtocols> &lt;preferServerCiphers>true&lt;/preferServerCiphers> &lt;/server> &lt;client> &lt;!-- Used for connecting to https dictionary source and secured Zookeeper communication --> &lt;loadDefaultCAFile>true&lt;/loadDefaultCAFile> &lt;cacheSessions>true&lt;/cacheSessions> &lt;disableProtocols>sslv2,sslv3&lt;/disableProtocols> &lt;preferServerCiphers>true&lt;/preferServerCiphers> &lt;!-- Use for self-signed: &lt;verificationMode>none&lt;/verificationMode> --> &lt;invalidCertificateHandler> &lt;!-- Use for self-signed: &lt;name>AcceptCertificateHandler&lt;/name> --> &lt;name>RejectCertificateHandler&lt;/name> &lt;/invalidCertificateHandler> &lt;/client> &lt;/openSSL> &lt;!-- Default root page on http[s] server. For example load UI from https://tabix.io/ when opening http://localhost:8123 --> &lt;!-- &lt;http_server_default_response>&lt;![CDATA[&lt;html ng-app="SMI2">&lt;head>&lt;base href="http://ui.tabix.io/">&lt;/head>&lt;body>&lt;div ui-view="" class="content-ui">&lt;/div>&lt;script src="http://loader.tabix.io/master.js">&lt;/script>&lt;/body>&lt;/html>]]>&lt;/http_server_default_response> --> &lt;!-- Maximum number of concurrent queries. --> &lt;max_concurrent_queries>100&lt;/max_concurrent_queries> &lt;!-- Maximum memory usage (resident set size) for server process. Zero value or unset means default. Default is "max_server_memory_usage_to_ram_ratio" of available physical RAM. If the value is larger than "max_server_memory_usage_to_ram_ratio" of available physical RAM, it will be cut down. The constraint is checked on query execution time. If a query tries to allocate memory and the current memory usage plus allocation is greater than specified threshold, exception will be thrown. It is not practical to set this constraint to small values like just a few gigabytes, because memory allocator will keep this amount of memory in caches and the server will deny service of queries. --> &lt;max_server_memory_usage>0&lt;/max_server_memory_usage> &lt;!-- Maximum number of threads in the Global thread pool. This will default to a maximum of 10000 threads if not specified. This setting will be useful in scenarios where there are a large number of distributed queries that are running concurrently but are idling most of the time, in which case a higher number of threads might be required. --> &lt;max_thread_pool_size>10000&lt;/max_thread_pool_size> &lt;!-- Number of workers to recycle connections in background (see also drain_timeout). If the pool is full, connection will be drained synchronously. --> &lt;!-- &lt;max_threads_for_connection_collector>10&lt;/max_threads_for_connection_collector> --> &lt;!-- On memory constrained environments you may have to set this to value larger than 1. --> &lt;max_server_memory_usage_to_ram_ratio>0.9&lt;/max_server_memory_usage_to_ram_ratio> &lt;!-- Simple server-wide memory profiler. Collect a stack trace at every peak allocation step (in bytes). Data will be stored in system.trace_log table with query_id = empty string. Zero means disabled. --> &lt;total_memory_profiler_step>4194304&lt;/total_memory_profiler_step> &lt;!-- Collect random allocations and deallocations and write them into system.trace_log with 'MemorySample' trace_type. The probability is for every alloc/free regardless to the size of the allocation. Note that sampling happens only when the amount of untracked memory exceeds the untracked memory limit, which is 4 MiB by default but can be lowered if 'total_memory_profiler_step' is lowered. You may want to set 'total_memory_profiler_step' to 1 for extra fine grained sampling. --> &lt;total_memory_tracker_sample_probability>0&lt;/total_memory_tracker_sample_probability> &lt;!-- Set limit on number of open files (default: maximum). This setting makes sense on Mac OS X because getrlimit() fails to retrieve correct maximum value. --> &lt;!-- &lt;max_open_files>262144&lt;/max_open_files> --> &lt;!-- Size of cache of uncompressed blocks of data, used in tables of MergeTree family. In bytes. Cache is single for server. Memory is allocated only on demand. Cache is used when 'use_uncompressed_cache' user setting turned on (off by default). Uncompressed cache is advantageous only for very short queries and in rare cases. Note: uncompressed cache can be pointless for lz4, because memory bandwidth is slower than multi-core decompression on some server configurations. Enabling it can sometimes paradoxically make queries slower. --> &lt;uncompressed_cache_size>8589934592&lt;/uncompressed_cache_size> &lt;!-- Approximate size of mark cache, used in tables of MergeTree family. In bytes. Cache is single for server. Memory is allocated only on demand. You should not lower this value. --> &lt;mark_cache_size>5368709120&lt;/mark_cache_size> &lt;!-- If you enable the `min_bytes_to_use_mmap_io` setting, the data in MergeTree tables can be read with mmap to avoid copying from kernel to userspace. It makes sense only for large files and helps only if data reside in page cache. To avoid frequent open/mmap/munmap/close calls (which are very expensive due to consequent page faults) and to reuse mappings from several threads and queries, the cache of mapped files is maintained. Its size is the number of mapped regions (usually equal to the number of mapped files). The amount of data in mapped files can be monitored in system.metrics, system.metric_log by the MMappedFiles, MMappedFileBytes metrics and in system.asynchronous_metrics, system.asynchronous_metrics_log by the MMapCacheCells metric, and also in system.events, system.processes, system.query_log, system.query_thread_log, system.query_views_log by the CreatedReadBufferMMap, CreatedReadBufferMMapFailed, MMappedFileCacheHits, MMappedFileCacheMisses events. Note that the amount of data in mapped files does not consume memory directly and is not accounted in query or server memory usage - because this memory can be discarded similar to OS page cache. The cache is dropped (the files are closed) automatically on removal of old parts in MergeTree, also it can be dropped manually by the SYSTEM DROP MMAP CACHE query. --> &lt;mmap_cache_size>1000&lt;/mmap_cache_size> &lt;!-- Cache size in bytes for compiled expressions.--> &lt;compiled_expression_cache_size>134217728&lt;/compiled_expression_cache_size> &lt;!-- Cache size in elements for compiled expressions.--> &lt;compiled_expression_cache_elements_size>10000&lt;/compiled_expression_cache_elements_size> &lt;!-- Path to data directory, with trailing slash. --> &lt;path>/var/lib/clickhouse/&lt;/path> &lt;!-- Path to temporary data for processing hard queries. --> &lt;tmp_path>/var/lib/clickhouse/tmp/&lt;/tmp_path> &lt;!-- Policy from the &lt;storage_configuration> for the temporary files. If not set &lt;tmp_path> is used, otherwise &lt;tmp_path> is ignored. Notes: - move_factor is ignored - keep_free_space_bytes is ignored - max_data_part_size_bytes is ignored - you must have exactly one volume in that policy --> &lt;!-- &lt;tmp_policy>tmp&lt;/tmp_policy> --> &lt;!-- Directory with user provided files that are accessible by 'file' table function. --> &lt;user_files_path>/var/lib/clickhouse/user_files/&lt;/user_files_path> &lt;!-- LDAP server definitions. --> &lt;ldap_servers> &lt;!-- List LDAP servers with their connection parameters here to later 1) use them as authenticators for dedicated local users, who have 'ldap' authentication mechanism specified instead of 'password', or to 2) use them as remote user directories. Parameters: host - LDAP server hostname or IP, this parameter is mandatory and cannot be empty. port - LDAP server port, default is 636 if enable_tls is set to true, 389 otherwise. bind_dn - template used to construct the DN to bind to. The resulting DN will be constructed by replacing all '{user_name}' substrings of the template with the actual user name during each authentication attempt. user_dn_detection - section with LDAP search parameters for detecting the actual user DN of the bound user. This is mainly used in search filters for further role mapping when the server is Active Directory. The resulting user DN will be used when replacing '{user_dn}' substrings wherever they are allowed. By default, user DN is set equal to bind DN, but once search is performed, it will be updated with to the actual detected user DN value. base_dn - template used to construct the base DN for the LDAP search. The resulting DN will be constructed by replacing all '{user_name}' and '{bind_dn}' substrings of the template with the actual user name and bind DN during the LDAP search. scope - scope of the LDAP search. Accepted values are: 'base', 'one_level', 'children', 'subtree' (the default). search_filter - template used to construct the search filter for the LDAP search. The resulting filter will be constructed by replacing all '{user_name}', '{bind_dn}', and '{base_dn}' substrings of the template with the actual user name, bind DN, and base DN during the LDAP search. Note, that the special characters must be escaped properly in XML. verification_cooldown - a period of time, in seconds, after a successful bind attempt, during which a user will be assumed to be successfully authenticated for all consecutive requests without contacting the LDAP server. Specify 0 (the default) to disable caching and force contacting the LDAP server for each authentication request. enable_tls - flag to trigger use of secure connection to the LDAP server. Specify 'no' for plain text (ldap://) protocol (not recommended). Specify 'yes' for LDAP over SSL/TLS (ldaps://) protocol (recommended, the default). Specify 'starttls' for legacy StartTLS protocol (plain text (ldap://) protocol, upgraded to TLS). tls_minimum_protocol_version - the minimum protocol version of SSL/TLS. Accepted values are: 'ssl2', 'ssl3', 'tls1.0', 'tls1.1', 'tls1.2' (the default). tls_require_cert - SSL/TLS peer certificate verification behavior. Accepted values are: 'never', 'allow', 'try', 'demand' (the default). tls_cert_file - path to certificate file. tls_key_file - path to certificate key file. tls_ca_cert_file - path to CA certificate file. tls_ca_cert_dir - path to the directory containing CA certificates. tls_cipher_suite - allowed cipher suite (in OpenSSL notation). Example: &lt;my_ldap_server> &lt;host>localhost&lt;/host> &lt;port>636&lt;/port> &lt;bind_dn>uid={user_name},ou=users,dc=example,dc=com&lt;/bind_dn> &lt;verification_cooldown>300&lt;/verification_cooldown> &lt;enable_tls>yes&lt;/enable_tls> &lt;tls_minimum_protocol_version>tls1.2&lt;/tls_minimum_protocol_version> &lt;tls_require_cert>demand&lt;/tls_require_cert> &lt;tls_cert_file>/path/to/tls_cert_file&lt;/tls_cert_file> &lt;tls_key_file>/path/to/tls_key_file&lt;/tls_key_file> &lt;tls_ca_cert_file>/path/to/tls_ca_cert_file&lt;/tls_ca_cert_file> &lt;tls_ca_cert_dir>/path/to/tls_ca_cert_dir&lt;/tls_ca_cert_dir> &lt;tls_cipher_suite>ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:AES256-GCM-SHA384&lt;/tls_cipher_suite> &lt;/my_ldap_server> Example (typical Active Directory with configured user DN detection for further role mapping): &lt;my_ad_server> &lt;host>localhost&lt;/host> &lt;port>389&lt;/port> &lt;bind_dn>EXAMPLE\{user_name}&lt;/bind_dn> &lt;user_dn_detection> &lt;base_dn>CN=Users,DC=example,DC=com&lt;/base_dn> &lt;search_filter>(&amp;amp;(objectClass=user)(sAMAccountName={user_name}))&lt;/search_filter> &lt;/user_dn_detection> &lt;enable_tls>no&lt;/enable_tls> &lt;/my_ad_server> --> &lt;/ldap_servers> &lt;!-- To enable Kerberos authentication support for HTTP requests (GSS-SPNEGO), for those users who are explicitly configured to authenticate via Kerberos, define a single 'kerberos' section here. Parameters: principal - canonical service principal name, that will be acquired and used when accepting security contexts. This parameter is optional, if omitted, the default principal will be used. This parameter cannot be specified together with 'realm' parameter. realm - a realm, that will be used to restrict authentication to only those requests whose initiator's realm matches it. This parameter is optional, if omitted, no additional filtering by realm will be applied. This parameter cannot be specified together with 'principal' parameter. Example: &lt;kerberos /> Example: &lt;kerberos> &lt;principal>HTTP/clickhouse.example.com@EXAMPLE.COM&lt;/principal> &lt;/kerberos> Example: &lt;kerberos> &lt;realm>EXAMPLE.COM&lt;/realm> &lt;/kerberos> --> &lt;!-- Sources to read users, roles, access rights, profiles of settings, quotas. --> &lt;user_directories> &lt;users_xml> &lt;!-- Path to configuration file with predefined users. --> &lt;path>users.xml&lt;/path> &lt;/users_xml> &lt;local_directory> &lt;!-- Path to folder where users created by SQL commands are stored. --> &lt;path>/var/lib/clickhouse/access/&lt;/path> &lt;/local_directory> &lt;!-- To add an LDAP server as a remote user directory of users that are not defined locally, define a single 'ldap' section with the following parameters: server - one of LDAP server names defined in 'ldap_servers' config section above. This parameter is mandatory and cannot be empty. roles - section with a list of locally defined roles that will be assigned to each user retrieved from the LDAP server. If no roles are specified here or assigned during role mapping (below), user will not be able to perform any actions after authentication. role_mapping - section with LDAP search parameters and mapping rules. When a user authenticates, while still bound to LDAP, an LDAP search is performed using search_filter and the name of the logged in user. For each entry found during that search, the value of the specified attribute is extracted. For each attribute value that has the specified prefix, the prefix is removed, and the rest of the value becomes the name of a local role defined in ClickHouse, which is expected to be created beforehand by CREATE ROLE command. There can be multiple 'role_mapping' sections defined inside the same 'ldap' section. All of them will be applied. base_dn - template used to construct the base DN for the LDAP search. The resulting DN will be constructed by replacing all '{user_name}', '{bind_dn}', and '{user_dn}' substrings of the template with the actual user name, bind DN, and user DN during each LDAP search. scope - scope of the LDAP search. Accepted values are: 'base', 'one_level', 'children', 'subtree' (the default). search_filter - template used to construct the search filter for the LDAP search. The resulting filter will be constructed by replacing all '{user_name}', '{bind_dn}', '{user_dn}', and '{base_dn}' substrings of the template with the actual user name, bind DN, user DN, and base DN during each LDAP search. Note, that the special characters must be escaped properly in XML. attribute - attribute name whose values will be returned by the LDAP search. 'cn', by default. prefix - prefix, that will be expected to be in front of each string in the original list of strings returned by the LDAP search. Prefix will be removed from the original strings and resulting strings will be treated as local role names. Empty, by default. Example: &lt;ldap> &lt;server>my_ldap_server&lt;/server> &lt;roles> &lt;my_local_role1 /> &lt;my_local_role2 /> &lt;/roles> &lt;role_mapping> &lt;base_dn>ou=groups,dc=example,dc=com&lt;/base_dn> &lt;scope>subtree&lt;/scope> &lt;search_filter>(&amp;amp;(objectClass=groupOfNames)(member={bind_dn}))&lt;/search_filter> &lt;attribute>cn&lt;/attribute> &lt;prefix>clickhouse_&lt;/prefix> &lt;/role_mapping> &lt;/ldap> Example (typical Active Directory with role mapping that relies on the detected user DN): &lt;ldap> &lt;server>my_ad_server&lt;/server> &lt;role_mapping> &lt;base_dn>CN=Users,DC=example,DC=com&lt;/base_dn> &lt;attribute>CN&lt;/attribute> &lt;scope>subtree&lt;/scope> &lt;search_filter>(&amp;amp;(objectClass=group)(member={user_dn}))&lt;/search_filter> &lt;prefix>clickhouse_&lt;/prefix> &lt;/role_mapping> &lt;/ldap> --> &lt;/user_directories> &lt;!-- Default profile of settings. --> &lt;default_profile>default&lt;/default_profile> &lt;!-- Comma-separated list of prefixes for user-defined settings. --> &lt;custom_settings_prefixes>&lt;/custom_settings_prefixes> &lt;!-- System profile of settings. This settings are used by internal processes (Distributed DDL worker and so on). --> &lt;!-- &lt;system_profile>default&lt;/system_profile> --> &lt;!-- Buffer profile of settings. This settings are used by Buffer storage to flush data to the underlying table. Default: used from system_profile directive. --> &lt;!-- &lt;buffer_profile>default&lt;/buffer_profile> --> &lt;!-- Default database. --> &lt;default_database>default&lt;/default_database> &lt;!-- Server time zone could be set here. Time zone is used when converting between String and DateTime types, when printing DateTime in text formats and parsing DateTime from text, it is used in date and time related functions, if specific time zone was not passed as an argument. Time zone is specified as identifier from IANA time zone database, like UTC or Africa/Abidjan. If not specified, system time zone at server startup is used. Please note, that server could display time zone alias instead of specified name. Example: W-SU is an alias for Europe/Moscow and Zulu is an alias for UTC. --> &lt;!-- &lt;timezone>Europe/Moscow&lt;/timezone> --> &lt;!-- You can specify umask here (see "man umask"). Server will apply it on startup. Number is always parsed as octal. Default umask is 027 (other users cannot read logs, data files, etc; group can only read). --> &lt;!-- &lt;umask>022&lt;/umask> --> &lt;!-- Perform mlockall after startup to lower first queries latency and to prevent clickhouse executable from being paged out under high IO load. Enabling this option is recommended but will lead to increased startup time for up to a few seconds. --> &lt;mlock_executable>true&lt;/mlock_executable> &lt;!-- Reallocate memory for machine code ("text") using huge pages. Highly experimental. --> &lt;remap_executable>false&lt;/remap_executable> &lt;![CDATA[ Uncomment below in order to use JDBC table engine and function. To install and run JDBC bridge in background: * [Debian/Ubuntu] export MVN_URL=https://repo1.maven.org/maven2/ru/yandex/clickhouse/clickhouse-jdbc-bridge export PKG_VER=$(curl -sL $MVN_URL/maven-metadata.xml | grep '&lt;release>' | sed -e 's|.*>\(.*\)&lt;.*|\1|') wget https://github.com/ClickHouse/clickhouse-jdbc-bridge/releases/download/v$PKG_VER/clickhouse-jdbc-bridge_$PKG_VER-1_all.deb apt install --no-install-recommends -f ./clickhouse-jdbc-bridge_$PKG_VER-1_all.deb clickhouse-jdbc-bridge & * [CentOS/RHEL] export MVN_URL=https://repo1.maven.org/maven2/ru/yandex/clickhouse/clickhouse-jdbc-bridge export PKG_VER=$(curl -sL $MVN_URL/maven-metadata.xml | grep '&lt;release>' | sed -e 's|.*>\(.*\)&lt;.*|\1|') wget https://github.com/ClickHouse/clickhouse-jdbc-bridge/releases/download/v$PKG_VER/clickhouse-jdbc-bridge-$PKG_VER-1.noarch.rpm yum localinstall -y clickhouse-jdbc-bridge-$PKG_VER-1.noarch.rpm clickhouse-jdbc-bridge & Please refer to https://github.com/ClickHouse/clickhouse-jdbc-bridge#usage for more information. ]]> &lt;!-- &lt;jdbc_bridge> &lt;host>127.0.0.1&lt;/host> &lt;port>9019&lt;/port> &lt;/jdbc_bridge> --> &lt;!-- Configuration of clusters that could be used in Distributed tables. https://clickhouse.com/docs/en/operations/table_engines/distributed/ --> &lt;remote_servers> &lt;default_cluster> &lt;shard> &lt;weight>1&lt;/weight> &lt;internal_replication>false&lt;/internal_replication> &lt;replica> &lt;host>ck1&lt;/host> &lt;port>9000&lt;/port> &lt;/replica> &lt;/shard> &lt;shard> &lt;weight>1&lt;/weight> &lt;internal_replication>false&lt;/internal_replication> &lt;replica> &lt;host>ck2&lt;/host> &lt;port>9000&lt;/port> &lt;/replica> &lt;/shard> &lt;shard> &lt;weight>1&lt;/weight> &lt;internal_replication>false&lt;/internal_replication> &lt;replica> &lt;host>ck3&lt;/host> &lt;port>9000&lt;/port> &lt;/replica> &lt;/shard> &lt;/default_cluster> &lt;/remote_servers> &lt;macros> &lt;replica>ck1&lt;/replica> &lt;shard>01&lt;/shard> &lt;layer>01&lt;/layer> &lt;/macros> &lt;!-- The list of hosts allowed to use in URL-related storage engines and table functions. If this section is not present in configuration, all hosts are allowed. --> &lt;!--&lt;remote_url_allow_hosts>--> &lt;!-- Host should be specified exactly as in URL. The name is checked before DNS resolution. Example: "yandex.ru", "yandex.ru." and "www.yandex.ru" are different hosts. If port is explicitly specified in URL, the host:port is checked as a whole. If host specified here without port, any port with this host allowed. "yandex.ru" -> "yandex.ru:443", "yandex.ru:80" etc. is allowed, but "yandex.ru:80" -> only "yandex.ru:80" is allowed. If the host is specified as IP address, it is checked as specified in URL. Example: "[2a02:6b8:a::a]". If there are redirects and support for redirects is enabled, every redirect (the Location field) is checked. Host should be specified using the host xml tag: &lt;host>yandex.ru&lt;/host> --> &lt;!-- Regular expression can be specified. RE2 engine is used for regexps. Regexps are not aligned: don't forget to add ^ and $. Also don't forget to escape dot (.) metacharacter (forgetting to do so is a common source of error). --> &lt;!--&lt;/remote_url_allow_hosts>--> &lt;!-- If element has 'incl' attribute, then for it's value will be used corresponding substitution from another file. By default, path to file with substitutions is /etc/metrika.xml. It could be changed in config in 'include_from' element. Values for substitutions are specified in /clickhouse/name_of_substitution elements in that file. --> &lt;!-- ZooKeeper is used to store metadata about replicas, when using Replicated tables. Optional. If you don't use replicated tables, you could omit that. See https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication/ --> &lt;zookeeper> &lt;node> &lt;host>zookeeper1&lt;/host> &lt;port>2181&lt;/port> &lt;/node> &lt;/zookeeper> &lt;!-- Substitutions for parameters of replicated tables. Optional. If you don't use replicated tables, you could omit that. See https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication/#creating-replicated-tables --> &lt;!-- &lt;macros> &lt;shard>01&lt;/shard> &lt;replica>example01-01-1&lt;/replica> &lt;/macros> --> &lt;!-- Reloading interval for embedded dictionaries, in seconds. Default: 3600. --> &lt;builtin_dictionaries_reload_interval>3600&lt;/builtin_dictionaries_reload_interval> &lt;!-- Maximum session timeout, in seconds. Default: 3600. --> &lt;max_session_timeout>3600&lt;/max_session_timeout> &lt;!-- Default session timeout, in seconds. Default: 60. --> &lt;default_session_timeout>60&lt;/default_session_timeout> &lt;!-- Sending data to Graphite for monitoring. Several sections can be defined. --> &lt;!-- interval - send every X second root_path - prefix for keys hostname_in_path - append hostname to root_path (default = true) metrics - send data from table system.metrics events - send data from table system.events asynchronous_metrics - send data from table system.asynchronous_metrics --> &lt;!-- &lt;graphite> &lt;host>localhost&lt;/host> &lt;port>42000&lt;/port> &lt;timeout>0.1&lt;/timeout> &lt;interval>60&lt;/interval> &lt;root_path>one_min&lt;/root_path> &lt;hostname_in_path>true&lt;/hostname_in_path> &lt;metrics>true&lt;/metrics> &lt;events>true&lt;/events> &lt;events_cumulative>false&lt;/events_cumulative> &lt;asynchronous_metrics>true&lt;/asynchronous_metrics> &lt;/graphite> &lt;graphite> &lt;host>localhost&lt;/host> &lt;port>42000&lt;/port> &lt;timeout>0.1&lt;/timeout> &lt;interval>1&lt;/interval> &lt;root_path>one_sec&lt;/root_path> &lt;metrics>true&lt;/metrics> &lt;events>true&lt;/events> &lt;events_cumulative>false&lt;/events_cumulative> &lt;asynchronous_metrics>false&lt;/asynchronous_metrics> &lt;/graphite> --> &lt;!-- Serve endpoint for Prometheus monitoring. --> &lt;!-- endpoint - mertics path (relative to root, statring with "/") port - port to setup server. If not defined or 0 than http_port used metrics - send data from table system.metrics events - send data from table system.events asynchronous_metrics - send data from table system.asynchronous_metrics status_info - send data from different component from CH, ex: Dictionaries status --> &lt;!-- &lt;prometheus> &lt;endpoint>/metrics&lt;/endpoint> &lt;port>9363&lt;/port> &lt;metrics>true&lt;/metrics> &lt;events>true&lt;/events> &lt;asynchronous_metrics>true&lt;/asynchronous_metrics> &lt;status_info>true&lt;/status_info> &lt;/prometheus> --> &lt;!-- Query log. Used only for queries with setting log_queries = 1. --> &lt;query_log> &lt;!-- What table to insert data. If table is not exist, it will be created. When query log structure is changed after system update, then old table will be renamed and new table will be created automatically. --> &lt;database>system&lt;/database> &lt;table>query_log&lt;/table> &lt;!-- PARTITION BY expr: https://clickhouse.com/docs/en/table_engines/mergetree-family/custom_partitioning_key/ Example: event_date toMonday(event_date) toYYYYMM(event_date) toStartOfHour(event_time) --> &lt;partition_by>toYYYYMM(event_date)&lt;/partition_by> &lt;!-- Table TTL specification: https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree/#mergetree-table-ttl Example: event_date + INTERVAL 1 WEEK event_date + INTERVAL 7 DAY DELETE event_date + INTERVAL 2 WEEK TO DISK 'bbb' &lt;ttl>event_date + INTERVAL 30 DAY DELETE&lt;/ttl> --> &lt;!-- Instead of partition_by, you can provide full engine expression (starting with ENGINE = ) with parameters, Example: &lt;engine>ENGINE = MergeTree PARTITION BY toYYYYMM(event_date) ORDER BY (event_date, event_time) SETTINGS index_granularity = 1024&lt;/engine> --> &lt;!-- Interval of flushing data. --> &lt;flush_interval_milliseconds>7500&lt;/flush_interval_milliseconds> &lt;/query_log> &lt;!-- Trace log. Stores stack traces collected by query profilers. See query_profiler_real_time_period_ns and query_profiler_cpu_time_period_ns settings. --> &lt;trace_log> &lt;database>system&lt;/database> &lt;table>trace_log&lt;/table> &lt;partition_by>toYYYYMM(event_date)&lt;/partition_by> &lt;flush_interval_milliseconds>7500&lt;/flush_interval_milliseconds> &lt;/trace_log> &lt;!-- Query thread log. Has information about all threads participated in query execution. Used only for queries with setting log_query_threads = 1. --> &lt;query_thread_log> &lt;database>system&lt;/database> &lt;table>query_thread_log&lt;/table> &lt;partition_by>toYYYYMM(event_date)&lt;/partition_by> &lt;flush_interval_milliseconds>7500&lt;/flush_interval_milliseconds> &lt;/query_thread_log> &lt;!-- Query views log. Has information about all dependent views associated with a query. Used only for queries with setting log_query_views = 1. --> &lt;query_views_log> &lt;database>system&lt;/database> &lt;table>query_views_log&lt;/table> &lt;partition_by>toYYYYMM(event_date)&lt;/partition_by> &lt;flush_interval_milliseconds>7500&lt;/flush_interval_milliseconds> &lt;/query_views_log> &lt;!-- Uncomment if use part log. Part log contains information about all actions with parts in MergeTree tables (creation, deletion, merges, downloads).--> &lt;part_log> &lt;database>system&lt;/database> &lt;table>part_log&lt;/table> &lt;partition_by>toYYYYMM(event_date)&lt;/partition_by> &lt;flush_interval_milliseconds>7500&lt;/flush_interval_milliseconds> &lt;/part_log> &lt;!-- Uncomment to write text log into table. Text log contains all information from usual server log but stores it in structured and efficient way. The level of the messages that goes to the table can be limited (&lt;level>), if not specified all messages will go to the table. &lt;text_log> &lt;database>system&lt;/database> &lt;table>text_log&lt;/table> &lt;flush_interval_milliseconds>7500&lt;/flush_interval_milliseconds> &lt;level>&lt;/level> &lt;/text_log> --> &lt;!-- Metric log contains rows with current values of ProfileEvents, CurrentMetrics collected with "collect_interval_milliseconds" interval. --> &lt;metric_log> &lt;database>system&lt;/database> &lt;table>metric_log&lt;/table> &lt;flush_interval_milliseconds>7500&lt;/flush_interval_milliseconds> &lt;collect_interval_milliseconds>1000&lt;/collect_interval_milliseconds> &lt;/metric_log> &lt;!-- Asynchronous metric log contains values of metrics from system.asynchronous_metrics. --> &lt;asynchronous_metric_log> &lt;database>system&lt;/database> &lt;table>asynchronous_metric_log&lt;/table> &lt;!-- Asynchronous metrics are updated once a minute, so there is no need to flush more often. --> &lt;flush_interval_milliseconds>7000&lt;/flush_interval_milliseconds> &lt;/asynchronous_metric_log> &lt;!-- OpenTelemetry log contains OpenTelemetry trace spans. --> &lt;opentelemetry_span_log> &lt;!-- The default table creation code is insufficient, this &lt;engine> spec is a workaround. There is no 'event_time' for this log, but two times, start and finish. It is sorted by finish time, to avoid inserting data too far away in the past (probably we can sometimes insert a span that is seconds earlier than the last span in the table, due to a race between several spans inserted in parallel). This gives the spans a global order that we can use to e.g. retry insertion into some external system. --> &lt;engine> engine MergeTree partition by toYYYYMM(finish_date) order by (finish_date, finish_time_us, trace_id) &lt;/engine> &lt;database>system&lt;/database> &lt;table>opentelemetry_span_log&lt;/table> &lt;flush_interval_milliseconds>7500&lt;/flush_interval_milliseconds> &lt;/opentelemetry_span_log> &lt;!-- Crash log. Stores stack traces for fatal errors. This table is normally empty. --> &lt;crash_log> &lt;database>system&lt;/database> &lt;table>crash_log&lt;/table> &lt;partition_by/> &lt;flush_interval_milliseconds>1000&lt;/flush_interval_milliseconds> &lt;/crash_log> &lt;!-- Session log. Stores user log in (successful or not) and log out events. --> &lt;session_log> &lt;database>system&lt;/database> &lt;table>session_log&lt;/table> &lt;partition_by>toYYYYMM(event_date)&lt;/partition_by> &lt;flush_interval_milliseconds>7500&lt;/flush_interval_milliseconds> &lt;/session_log> &lt;!-- Parameters for embedded dictionaries, used in Yandex.Metrica. See https://clickhouse.com/docs/en/dicts/internal_dicts/ --> &lt;!-- Path to file with region hierarchy. --> &lt;!-- &lt;path_to_regions_hierarchy_file>/opt/geo/regions_hierarchy.txt&lt;/path_to_regions_hierarchy_file> --> &lt;!-- Path to directory with files containing names of regions --> &lt;!-- &lt;path_to_regions_names_files>/opt/geo/&lt;/path_to_regions_names_files> --> &lt;!-- &lt;top_level_domains_path>/var/lib/clickhouse/top_level_domains/&lt;/top_level_domains_path> --> &lt;!-- Custom TLD lists. Format: &lt;name>/path/to/file&lt;/name> Changes will not be applied w/o server restart. Path to the list is under top_level_domains_path (see above). --> &lt;top_level_domains_lists> &lt;!-- &lt;public_suffix_list>/path/to/public_suffix_list.dat&lt;/public_suffix_list> --> &lt;/top_level_domains_lists> &lt;!-- Configuration of external dictionaries. See: https://clickhouse.com/docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts --> &lt;dictionaries_config>*_dictionary.xml&lt;/dictionaries_config> &lt;!-- Configuration of user defined executable functions --> &lt;user_defined_executable_functions_config>*_function.xml&lt;/user_defined_executable_functions_config> &lt;!-- Uncomment if you want data to be compressed 30-100% better. Don't do that if you just started using ClickHouse. --> &lt;!-- &lt;compression> &lt;!- - Set of variants. Checked in order. Last matching case wins. If nothing matches, lz4 will be used. - -> &lt;case> &lt;!- - Conditions. All must be satisfied. Some conditions may be omitted. - -> &lt;min_part_size>10000000000&lt;/min_part_size> &lt;!- - Min part size in bytes. - -> &lt;min_part_size_ratio>0.01&lt;/min_part_size_ratio> &lt;!- - Min size of part relative to whole table size. - -> &lt;!- - What compression method to use. - -> &lt;method>zstd&lt;/method> &lt;/case> &lt;/compression> --> &lt;!-- Configuration of encryption. The server executes a command to obtain an encryption key at startup if such a command is defined, or encryption codecs will be disabled otherwise. The command is executed through /bin/sh and is expected to write a Base64-encoded key to the stdout. --> &lt;encryption_codecs> &lt;!-- aes_128_gcm_siv --> &lt;!-- Example of getting hex key from env --> &lt;!-- the code should use this key and throw an exception if its length is not 16 bytes --> &lt;!--key_hex from_env="...">&lt;/key_hex --> &lt;!-- Example of multiple hex keys. They can be imported from env or be written down in config--> &lt;!-- the code should use these keys and throw an exception if their length is not 16 bytes --> &lt;!-- key_hex id="0">...&lt;/key_hex --> &lt;!-- key_hex id="1" from_env="..">&lt;/key_hex --> &lt;!-- key_hex id="2">...&lt;/key_hex --> &lt;!-- current_key_id>2&lt;/current_key_id --> &lt;!-- Example of getting hex key from config --> &lt;!-- the code should use this key and throw an exception if its length is not 16 bytes --> &lt;!-- key>...&lt;/key --> &lt;!-- example of adding nonce --> &lt;!-- nonce>...&lt;/nonce --> &lt;!-- /aes_128_gcm_siv --> &lt;/encryption_codecs> &lt;!-- Allow to execute distributed DDL queries (CREATE, DROP, ALTER, RENAME) on cluster. Works only if ZooKeeper is enabled. Comment it if such functionality isn't required. --> &lt;distributed_ddl> &lt;!-- Path in ZooKeeper to queue with DDL queries --> &lt;path>/clickhouse/task_queue/ddl&lt;/path> &lt;!-- Settings from this profile will be used to execute DDL queries --> &lt;!-- &lt;profile>default&lt;/profile> --> &lt;!-- Controls how much ON CLUSTER queries can be run simultaneously. --> &lt;!-- &lt;pool_size>1&lt;/pool_size> --> &lt;!-- Cleanup settings (active tasks will not be removed) --> &lt;!-- Controls task TTL (default 1 week) --> &lt;!-- &lt;task_max_lifetime>604800&lt;/task_max_lifetime> --> &lt;!-- Controls how often cleanup should be performed (in seconds) --> &lt;!-- &lt;cleanup_delay_period>60&lt;/cleanup_delay_period> --> &lt;!-- Controls how many tasks could be in the queue --> &lt;!-- &lt;max_tasks_in_queue>1000&lt;/max_tasks_in_queue> --> &lt;/distributed_ddl> &lt;!-- Settings to fine tune MergeTree tables. See documentation in source code, in MergeTreeSettings.h --> &lt;!-- &lt;merge_tree> &lt;max_suspicious_broken_parts>5&lt;/max_suspicious_broken_parts> &lt;/merge_tree> --> &lt;!-- Protection from accidental DROP. If size of a MergeTree table is greater than max_table_size_to_drop (in bytes) than table could not be dropped with any DROP query. If you want do delete one table and don't want to change clickhouse-server config, you could create special file &lt;clickhouse-path>/flags/force_drop_table and make DROP once. By default max_table_size_to_drop is 50GB; max_table_size_to_drop=0 allows to DROP any tables. The same for max_partition_size_to_drop. Uncomment to disable protection. --> &lt;!-- &lt;max_table_size_to_drop>0&lt;/max_table_size_to_drop> --> &lt;!-- &lt;max_partition_size_to_drop>0&lt;/max_partition_size_to_drop> --> &lt;!-- Example of parameters for GraphiteMergeTree table engine --> &lt;graphite_rollup_example> &lt;pattern> &lt;regexp>click_cost&lt;/regexp> &lt;function>any&lt;/function> &lt;retention> &lt;age>0&lt;/age> &lt;precision>3600&lt;/precision> &lt;/retention> &lt;retention> &lt;age>86400&lt;/age> &lt;precision>60&lt;/precision> &lt;/retention> &lt;/pattern> &lt;default> &lt;function>max&lt;/function> &lt;retention> &lt;age>0&lt;/age> &lt;precision>60&lt;/precision> &lt;/retention> &lt;retention> &lt;age>3600&lt;/age> &lt;precision>300&lt;/precision> &lt;/retention> &lt;retention> &lt;age>86400&lt;/age> &lt;precision>3600&lt;/precision> &lt;/retention> &lt;/default> &lt;/graphite_rollup_example> &lt;!-- Directory in &lt;clickhouse-path> containing schema files for various input formats. The directory will be created if it doesn't exist. --> &lt;format_schema_path>/var/lib/clickhouse/format_schemas/&lt;/format_schema_path> &lt;!-- Default query masking rules, matching lines would be replaced with something else in the logs (both text logs and system.query_log). name - name for the rule (optional) regexp - RE2 compatible regular expression (mandatory) replace - substitution string for sensitive data (optional, by default - six asterisks) --> &lt;query_masking_rules> &lt;rule> &lt;name>hide encrypt/decrypt arguments&lt;/name> &lt;regexp>((?:aes_)?(?:encrypt|decrypt)(?:_mysql)?)\s*\(\s*(?:'(?:\\'|.)+'|.*?)\s*\)&lt;/regexp> &lt;!-- or more secure, but also more invasive: (aes_\w+)\s*\(.*\) --> &lt;replace>\1(???)&lt;/replace> &lt;/rule> &lt;/query_masking_rules> &lt;!-- Uncomment to use custom http handlers. rules are checked from top to bottom, first match runs the handler url - to match request URL, you can use 'regex:' prefix to use regex match(optional) methods - to match request method, you can use commas to separate multiple method matches(optional) headers - to match request headers, match each child element(child element name is header name), you can use 'regex:' prefix to use regex match(optional) handler is request handler type - supported types: static, dynamic_query_handler, predefined_query_handler query - use with predefined_query_handler type, executes query when the handler is called query_param_name - use with dynamic_query_handler type, extracts and executes the value corresponding to the &lt;query_param_name> value in HTTP request params status - use with static type, response status code content_type - use with static type, response content-type response_content - use with static type, Response content sent to client, when using the prefix 'file://' or 'config://', find the content from the file or configuration send to client. &lt;http_handlers> &lt;rule> &lt;url>/&lt;/url> &lt;methods>POST,GET&lt;/methods> &lt;headers>&lt;pragma>no-cache&lt;/pragma>&lt;/headers> &lt;handler> &lt;type>dynamic_query_handler&lt;/type> &lt;query_param_name>query&lt;/query_param_name> &lt;/handler> &lt;/rule> &lt;rule> &lt;url>/predefined_query&lt;/url> &lt;methods>POST,GET&lt;/methods> &lt;handler> &lt;type>predefined_query_handler&lt;/type> &lt;query>SELECT * FROM system.settings&lt;/query> &lt;/handler> &lt;/rule> &lt;rule> &lt;handler> &lt;type>static&lt;/type> &lt;status>200&lt;/status> &lt;content_type>text/plain; charset=UTF-8&lt;/content_type> &lt;response_content>config://http_server_default_response&lt;/response_content> &lt;/handler> &lt;/rule> &lt;/http_handlers> --> &lt;send_crash_reports> &lt;!-- Changing &lt;enabled> to true allows sending crash reports to --> &lt;!-- the ClickHouse core developers team via Sentry https://sentry.io --> &lt;!-- Doing so at least in pre-production environments is highly appreciated --> &lt;enabled>false&lt;/enabled> &lt;!-- Change &lt;anonymize> to true if you don't feel comfortable attaching the server hostname to the crash report --> &lt;anonymize>false&lt;/anonymize> &lt;!-- Default endpoint s hould be changed to different Sentry DSN only if you have --> &lt;!-- some in-house engineers or hired consultants who're going to debug ClickHouse issues for you --> &lt;endpoint>&lt;/endpoint> &lt;/send_crash_reports> &lt;!-- Uncomment to disable ClickHouse internal DNS caching. --> &lt;!-- &lt;disable_internal_dns_cache>1&lt;/disable_internal_dns_cache> --> &lt;!-- You can also configure rocksdb like this: --> &lt;!-- &lt;rocksdb> &lt;options> &lt;max_background_jobs>8&lt;/max_background_jobs> &lt;/options> &lt;column_family_options> &lt;num_levels>2&lt;/num_levels> &lt;/column_family_options> &lt;tables> &lt;table> &lt;name>TABLE&lt;/name> &lt;options> &lt;max_background_jobs>8&lt;/max_background_jobs> &lt;/options> &lt;column_family_options> &lt;num_levels>2&lt;/num_levels> &lt;/column_family_options> &lt;/table> &lt;/tables> &lt;/rocksdb> --> &lt;timezone>Asia/Shanghai&lt;/timezone> &lt;/clickhouse> 启动命令 docker-compose up -d TODO 现在配置的 zookeeper 还是单机部署的，有空看看部署个zookeeper集群怎么配置
...</p></div><footer class=entry-footer><span title='2022-08-12 17:30:00 +0800 CST'>2022-08-12 17:30:00</span>&nbsp;·&nbsp;26 min&nbsp;·&nbsp;5510 words&nbsp;·&nbsp;johnpoint</footer><a class=entry-link aria-label="post link to 使用 docker-compose 搭建 clickhouse 集群" href=https://blog.lvcshu.com/2022/08/12/deploy-clickhouse-by-docker-compose/></a></article><article class=post-entry><header class=entry-header><h2>Go 实现瑞士轮排列算法</h2></header><div class=entry-content><p>工作原因接触到了瑞士轮这种赛制，记录一下瑞士轮比赛对手编排的算法
瑞士轮有两个规则
选择积分相近的对手进行比赛 不会重复比赛 写出来的算法如下:
type player struct { Id int64 Score int64 Opponent map[int64]struct{} // 曾经遇到过的对手 } // pickTablePlayer 计算瑞士轮比赛排列 func pickTablePlayer(players []int64, playerOpponentMap map[int64]map[int64]struct{}) ([]int64, bool) { if len(players) &lt; 2 { return players, true } whitePlayer := players[0] opponentMap, _ := playerOpponentMap[whitePlayer] for i := range players { if i != 0 { // 判断是否已经比过 if _, has := opponentMap[players[i]]; !has { // 选中 res := make([]int64, 2) res[0] = whitePlayer res[1] = players[i] // 组装剩下排序的数据 var nextRound []int64 nextRound = append(nextRound, players[1:i]...) nextRound = append(nextRound, players[i+1:]...) pick, ok := pickTablePlayer(nextRound, playerOpponentMap) // 进行下一轮排序 if ok { return append(res, pick...), true // 成功，结果上浮 } } } } return nil, false // 失败，上层重算 } func CreateSwissRound(players []player) (playerBattleList [][]int64, emptyPlayer int64, ok bool) { ok = true // 判断轮空选手 total := len(players) if total%2 != 0 { emptyPlayer = players[total-1].Id players = players[:total] } // 转换数据结构 var playerIds []int64 var playerOpponentMap = make(map[int64]map[int64]struct{}) for _, v := range players { playerIds = append(playerIds, v.Id) if _, has := playerOpponentMap[v.Id]; !has { playerOpponentMap[v.Id] = v.Opponent } } // 计算比赛排序 playerList, ok := pickTablePlayer(playerIds, playerOpponentMap) if !ok { return playerBattleList, emptyPlayer, ok } // 转换为二维数组 for i := 0; i &lt; len(playerList)/2; i++ { playerBattleList = append(playerBattleList, []int64{ playerList[i*2], playerList[i*2+1], }) } return }</p></div><footer class=entry-footer><span title='2022-07-29 10:00:00 +0800 CST'>2022-07-29 10:00:00</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;214 words&nbsp;·&nbsp;johnpoint</footer><a class=entry-link aria-label="post link to Go 实现瑞士轮排列算法" href=https://blog.lvcshu.com/2022/07/29/%E7%91%9E%E5%A3%AB%E8%BD%AE/></a></article><article class=post-entry><header class=entry-header><h2>Oneplus 8T 刷入 LineageOS</h2></header><div class=entry-content><p>劳动节来给博客除除草！
自从一加手机社区发布了官方公告说 Android 12 正式版本出来了之后我就一直在等系统更新的推送，谁知道从4月12日公告出来到今天我都没有收到推送，再加上一加的在 Android 12 后 HOS 会切换成 ColorOS，类原生的特点就没有了，OOS 虽说还会持续维护，但是我既然都用 OOS 了我为啥不自己刷个更加原生的系统呢？比如说 LineageOS。
前期准备 说干就干，先去官网看下有没有支持，芜湖，有支持而且看了下文档还蛮完善的，备份好微信(这个手机里面唯一没有同步功能的app)的数据就打算开始刷机了。
开刷 刷机的过程官方文档已经非常完善了，在这里不重复赘述。
一些要注意的小问题 GAPPS GAPPS 一定要在首次启动系统之前刷入，不然就要双清，之前辛苦配置的东西都无了
SafetyNet 在刷好系统之后，我自然是想打开 ingress 玩下，然后折腾了半天，一直在提醒 ”ingress 需要安全登录“，一开始还以为是代理的问题，疯狂切换代理都没有用，后来查到这个讨论发现是 SafetyNet 的问题，于是 Magisk 刷入了 MagiskHide Props Config、Universal SafetyNet Fix 两个模块解决了这个问题
Universal SafetyNet Fix 这个模块无需任何配置，直接刷入即可生效 MagiskHide Props Config 这个则需要在shell执行指令 props 按照提示选择即可。 相机 自带的相机 app 太拉了，直接停用，在 Google Camera Port 下载了个最新版本的相机，以及挑了个推荐的配置文件。
使用感受 原生的系统真是舒服啊，没有了一些有的没有的app，动画啥的感觉要比HOS要好。高帧率、AOD、蓝牙HD音频编码、屏下指纹这些都没有啥大问题。
甚至有些之前在HOS上面没有体验过的特性，比如说锁屏音乐可视化 总的来说挺满意的，再看看后续使用的过程中有没有啥坑了，就这样。</p></div><footer class=entry-footer><span title='2022-05-01 11:51:00 +0800 CST'>2022-05-01 11:51:00</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;61 words&nbsp;·&nbsp;johnpoint</footer><a class=entry-link aria-label="post link to Oneplus 8T 刷入 LineageOS" href=https://blog.lvcshu.com/2022/05/01/oneplus-8t-%E5%88%B7%E5%85%A5-lineageos/></a></article><article class=post-entry><header class=entry-header><h2>博客及相关服务部署更新</h2></header><div class=entry-content><p>部署流程更新 众所周知，之前我的博客以及图片托管的服务是通过硬核的DNS分区域解析来实现流量的就近调度，而多个服务器之间的资源文件我一般使用syncthing来进行同步，这个方法显得比较粗糙，但是还挺有效的。最近折腾了下 cloudflare worker 感觉还不错，于是就把博客以及图片托管迁移到了 cloudflare worker 上来，这样一来可以白嫖到不错的全球加速(除中国大陆)，而且使用 workers 进行部署可以省下不少的服务器资源。
这个就是我之前的发布工作流程
在这次部署流程更新后他变成了这样，博客由于是 public 代码仓库，所以我的自动构建直接使用了 github action，只要基于之前的配置改改加上 publish 到 cloudflare workers 的 step 就可以直接用了。
而静态文件托管的我一直是存在自建的 gitea 上面，所以配了个 drone 来执行自动 publish 到 cloudflare workers 的工作，总而言之也是挺香的。
5月1日更新 又更改了下，用上了 vercel 的服务，实测这个服务提供的 cdn 要比 cloudflare 在中国大陆访问的快些。 博客主题更新 博客主题也好久没有更新了，这次更新主要是加上了几个能够改进访问体验的 feature。
添加了首页巨幕以及文章头图图片下面的背景颜色，能够自定义在图片还没有加载出来之前显示的颜色 将博客底部版权信息部分的版本号放进了一个js里面，这样就减少了版本号修改时候需要修改的位置 将原来的文章无头图从几张图片中随机选择一张作为头图改成了随机选择一个颜色作为头图 颜色随机取值这里，是取值RGB都在 110-87之间的值，这里的颜色饱和度比较低，看起来比较舒服</p></div><footer class=entry-footer><span title='2022-04-17 00:40:00 +0800 CST'>2022-04-17 00:40:00</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;49 words&nbsp;·&nbsp;johnpoint</footer><a class=entry-link aria-label="post link to 博客及相关服务部署更新" href=https://blog.lvcshu.com/2022/04/17/%E5%8D%9A%E5%AE%A2%E5%8F%8A%E7%9B%B8%E5%85%B3%E6%9C%8D%E5%8A%A1%E9%83%A8%E7%BD%B2%E6%9B%B4%E6%96%B0/></a></article><article class=post-entry><header class=entry-header><h2>博客迁移到cloudflare踩坑</h2></header><div class=entry-content><p>好久不见，新年开始一直忙着毕业设计和实习找新的工作，一直没有空去将一些折腾过的东西记录成为博客，最近在写毕业设计的间隙终于对博客进行了一波优化，顺便写篇博客记录一下。
在过去的部署中，博客一直是采用多节点部署并且通过dnspod的分地区解析做流量调度，将流量解析到尽量近的节点来尽量保证博客访问的速度。而多个节点之间的博客文件同步一开始用的是定时任务从github上面更新，后来改成了使用syncthing进行同步，这种方法看起来比较蠢，但是也持续的保证了我的博客在这两年期间的顺畅访问。
最近在翻sukka大佬的博客过程中，看到了 cloudflare worker 可以联合 kv 存储用来部署静态网站，于是乎我就先将自托管的图片提供服务(就是一个存了图片的http服务)，部署到了 cloudflare 上面，测试速度以及延迟也相当不错，所以就想彻底的把博客这一套东西完全迁移到 cloudflare 上面去。
这样就能保证我博客在我不主动折腾的情况下保证极高的可靠性以及相对不错的响应速度。
404页面异常 在 worker-site/index.js 文件中，有一段逻辑是控制在url无法获取到文件的时候返回 /404.html。但是一部署上去我就发现了不对劲，这个404页面直接源代码显示，并没有被浏览器渲染出来。
经过F12大法查看network的response知道返回的数据中缺少了个指定响应数据格式的header。
修复这个情况只需要在返回响应的时候加上相应的header即可，代码修改可以参考我提的PR
参考链接 将 Hexo 部署到 Cloudflare Workers Site 上的趟坑记录</p></div><footer class=entry-footer><span title='2022-03-25 13:11:29 +0800 CST'>2022-03-25 13:11:29</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;28 words&nbsp;·&nbsp;johnpoint</footer><a class=entry-link aria-label="post link to 博客迁移到cloudflare踩坑" href=https://blog.lvcshu.com/2022/03/25/blog-on-cloudflare-workers/></a></article><article class=post-entry><header class=entry-header><h2>2021 年度简报</h2></header><div class=entry-content><p></p></div><footer class=entry-footer><span title='2021-12-26 15:11:29 +0800 CST'>2021-12-26 15:11:29</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;69 words&nbsp;·&nbsp;johnpoint</footer><a class=entry-link aria-label="post link to 2021 年度简报" href=https://blog.lvcshu.com/2021/12/26/2021-year-summary/></a></article><article class=post-entry><header class=entry-header><h2>使用 Zerotier 异地组内网</h2></header><div class=entry-content><p>众所周知，世界上的 ipv4 地址已经快要枯竭 (错了就当我在瞎掰)，所以在国内不是所有的运营商都拥有公网 IP 地址，同时还有许多奇奇怪怪的需求 (比如说想异地联机打红警之类的)，想要实现异地组建内网的效果不得不要采取一些特别手段进行组网。其实异地组网的文章有不少，比如柠檬雨大佬的这篇文章 《异地也要玩局域网——使用N2N，实现异地服务器快速组建内网》，中间就用到了一个叫做 N2N 的软件来进行隧道的建立。但是吧，我在想用 N2N 进行组网的时候，遇到了不少问题，首先就是 windows 客户端 v2 版本的我只找到了一个网络博主自己编译的版本，而且国内服务器在获取客户端的时候非常慢。
其中一度不想折腾想着使用那些现成的局域网对战平台来进行游戏 (对就是浩方) 但是下载注册 (这个平台注册居然还要身份证，就离谱，虽然我是用生成的身份证号码注册的) 之后，登录这边一直卡死在 loading，无奈我只能再看看有没有其他的方案。
在一位 (非常非常非常想玩红警3的) 朋友的建议下，我去研究了下 ZeroTier 这个异地组网的解决方案，发现挺方便的。
但是问题来了，在我用上了 ZeroTier 之后，隧道倒是建立了，但是不知道为何，有两个节点相互 ping 不通</p></div><footer class=entry-footer><span title='2021-11-24 16:25:00 +0800 CST'>2021-11-24 16:25:00</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;32 words&nbsp;·&nbsp;johnpoint</footer><a class=entry-link aria-label="post link to 使用 Zerotier 异地组内网" href=https://blog.lvcshu.com/2021/11/24/zerotier-%E6%9E%84%E5%BB%BA%E5%86%85%E7%BD%91/></a></article><article class=post-entry><header class=entry-header><h2>用于 gnet 的 Protocol buffers 编解码器</h2></header><div class=entry-content><p>要写一个 TCP 服务端，实现处理在纯 TCP 流中传输的 Protocol buffers 数据。网络框架很早就选好了，用性能杰出的 gnet，问题是 gnet 的示例库里面没有直接解析纯 Protocol buffers 的编解码器，于是乎只能自己动手了…
协议分析 从 TCP 流里面传过来的是经过简单处理的 Protocol buffers 数据，他在数据的头携带了这个数据包的长度信息，像是这样
[ 头 ][ 数据 ][ 头 ][ 数据 ][ 头 ][ 数据 ][ 头 ][ 数据 ][ 头 ][ 数据 ] 调用 golang 的 proto 官方库中的 func DecodeVarint(b []byte) (uint64, int) 方法可以从数据中拿到两个值，分别是 数据的完整长度、标明数据长度的头信息的长度。
由于没有特定的协议在包与包之间进行明显的划分，所以得用他的头数据来进行分包。
解码器 // 储存连接内的相关信息 type DataStruct struct { fullLength int lenNumLength int fullData []byte } func (d *Codec) Decode(c gnet.Conn) ([]byte, error) { ctx, ok := c.Context().(context.Context) if !ok { err := c.Close() if err != nil { return nil, nil } } // 从上下文里面拿出这个连接的编解码器储存 struct r, ok := ctx.Value("codec").(DataStruct) if !ok { err := c.Close() if err != nil { return nil, nil } } // 读取缓冲区内的所有信息 bytes := c.Read() // 判断是否已经开始读取包 if len(r.fullData) == 0 { // 调用函数获取头中带的信息 var fullLength uint64 fullLength, r.lenNumLength = proto.DecodeVarint(bytes) r.fullLength = int(fullLength) fmt.Println(r.fullLength, r.lenNumLength) if r.fullLength == 0 { return nil, nil } } // 拿到当前时间已经被储存进 struct 的数据的长度 fullDataLong := len(r.fullData) // 把读到的数据一把梭全部拼进 fullData r.fullData = append(r.fullData, bytes...) // 判断长度是否符合要求 if len(r.fullData) >= r.fullLength+r.lenNumLength { c.ShiftN(r.fullLength + r.lenNumLength - fullDataLong) // 截取有效的数据 res := r.fullData[r.lenNumLength : r.fullLength+r.lenNumLength] // 连接的缓存清空 r.fullData = []byte{} ctx = context.WithValue(ctx, "codec", r) c.SetContext(ctx) return res, nil } // 移动读取指针 c.ShiftN(len(bytes)) ctx = context.WithValue(ctx, "codec", r) c.SetContext(ctx) return nil, nil } 上面那种解码方式是目前看运行状况来说暂时没有出现问题的方法，下面那一种则比较节省内存，两种解码方式区别主要是在于调用的 Read 函数不同，前者是把 gnet 的 ring buffer 里面的内容全部读取出来，而后者是先把头读取出来，拿到了完整的数据长度信息之后调用 ReadN 函数直接准确的将包体取出。
...</p></div><footer class=entry-footer><span title='2021-09-17 18:30:00 +0800 CST'>2021-09-17 18:30:00</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;692 words&nbsp;·&nbsp;johnpoint</footer><a class=entry-link aria-label="post link to 用于 gnet 的 Protocol buffers 编解码器" href=https://blog.lvcshu.com/2021/09/17/tcp-protocol-buffers-codec/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://blog.lvcshu.com/>«&nbsp;Prev&nbsp;
</a><a class=next href=https://blog.lvcshu.com/page/3/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=https://blog.lvcshu.com/>johnpoint's blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script async src=https://umami.uipo.cc/script.js data-website-id=7319ce07-a672-4d99-bf07-9ebb227d449d></script></body></html>