<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>使用 logstash 采集来自腾讯云 tke 的日志 | johnpoint's blog</title><meta name=keywords content="logstash,腾讯云,笔记"><meta name=description content="前提 好久没有给博客除草了，正好最近折腾了下 logstash，记录一下。
为啥要用 logstash 呢，其实是因为在测试环境上面腾讯云 tke 的日志没有开启日志收集，所以在排查问题的时候会十分的痛苦，正好有空了就想着将日志抽出来放进 es 里面，方便以后排查问题，正好看到腾讯云的日志规则是允许将 pod 的 stdout 日志进行采集之后投递到 kafka 的，就小试了一下。
部署 logstash logstash 我选择使用 docker-compose 来进行快速的部署。
以下是部署流程，参考自 deviantony/docker-elk 项目
创建目录 mkdir logstash/config logstash/pipeline -p 创建环境变量 路径 .env
ELASTIC_VERSION=8.7.1 LOGSTASH_INTERNAL_PASSWORD='changeme' 创建 Dockerfile 路径 logstasg/Dockerfile
ARG ELASTIC_VERSION # https://www.docker.elastic.co/ FROM docker.elastic.co/logstash/logstash:${ELASTIC_VERSION} 配置文件 路径 logstash/config/logstash.yml
--- ## Default Logstash configuration from Logstash base image. ## https://github.com/elastic/logstash/blob/main/docker/data/logstash/config/logstash-full.yml # http.host: 0.0.0.0 node.name: logstash 路径 logstash/pipeline/logstash.conf
input { beats { port => 5044 } tcp { port => 50000 } } ## Add your filters / logstash plugins configuration here output { elasticsearch { hosts => &#34;elasticsearch:9200&#34; user => &#34;logstash_internal&#34; password => &#34;${LOGSTASH_INTERNAL_PASSWORD}&#34; index => &#34;logstash-%{+YYYY-MM-dd}&#34; } } 启动服务 version: '3."><meta name=author content="johnpoint"><link rel=canonical href=https://blog.lvcshu.com/2023/05/15/%E4%BD%BF%E7%94%A8-logstash-%E9%87%87%E9%9B%86%E6%9D%A5%E8%87%AA%E8%85%BE%E8%AE%AF%E4%BA%91-tke-%E7%9A%84%E6%97%A5%E5%BF%97/><link crossorigin=anonymous href=/assets/css/stylesheet.635203c1629fde69896622bdc0aee4210e2fa2ca161e2cc0cf7ea8ccf604149e.css integrity="sha256-Y1IDwWKf3mmJZiK9wK7kIQ4vosoWHizAz36ozPYEFJ4=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://blog.lvcshu.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://blog.lvcshu.com/favicon.ico><link rel=icon type=image/png sizes=32x32 href=https://blog.lvcshu.com/favicon.ico><link rel=apple-touch-icon href=https://blog.lvcshu.com/favicon.ico><link rel=mask-icon href=https://blog.lvcshu.com/favicon.ico><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="使用 logstash 采集来自腾讯云 tke 的日志"><meta property="og:description" content="前提 好久没有给博客除草了，正好最近折腾了下 logstash，记录一下。
为啥要用 logstash 呢，其实是因为在测试环境上面腾讯云 tke 的日志没有开启日志收集，所以在排查问题的时候会十分的痛苦，正好有空了就想着将日志抽出来放进 es 里面，方便以后排查问题，正好看到腾讯云的日志规则是允许将 pod 的 stdout 日志进行采集之后投递到 kafka 的，就小试了一下。
部署 logstash logstash 我选择使用 docker-compose 来进行快速的部署。
以下是部署流程，参考自 deviantony/docker-elk 项目
创建目录 mkdir logstash/config logstash/pipeline -p 创建环境变量 路径 .env
ELASTIC_VERSION=8.7.1 LOGSTASH_INTERNAL_PASSWORD='changeme' 创建 Dockerfile 路径 logstasg/Dockerfile
ARG ELASTIC_VERSION # https://www.docker.elastic.co/ FROM docker.elastic.co/logstash/logstash:${ELASTIC_VERSION} 配置文件 路径 logstash/config/logstash.yml
--- ## Default Logstash configuration from Logstash base image. ## https://github.com/elastic/logstash/blob/main/docker/data/logstash/config/logstash-full.yml # http.host: 0.0.0.0 node.name: logstash 路径 logstash/pipeline/logstash.conf
input { beats { port => 5044 } tcp { port => 50000 } } ## Add your filters / logstash plugins configuration here output { elasticsearch { hosts => &#34;elasticsearch:9200&#34; user => &#34;logstash_internal&#34; password => &#34;${LOGSTASH_INTERNAL_PASSWORD}&#34; index => &#34;logstash-%{+YYYY-MM-dd}&#34; } } 启动服务 version: '3."><meta property="og:type" content="article"><meta property="og:url" content="https://blog.lvcshu.com/2023/05/15/%E4%BD%BF%E7%94%A8-logstash-%E9%87%87%E9%9B%86%E6%9D%A5%E8%87%AA%E8%85%BE%E8%AE%AF%E4%BA%91-tke-%E7%9A%84%E6%97%A5%E5%BF%97/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-05-15T10:34:44+00:00"><meta property="article:modified_time" content="2023-05-15T10:34:44+00:00"><meta property="og:site_name" content="johnpoint's blog"><meta name=twitter:card content="summary"><meta name=twitter:title content="使用 logstash 采集来自腾讯云 tke 的日志"><meta name=twitter:description content="前提 好久没有给博客除草了，正好最近折腾了下 logstash，记录一下。
为啥要用 logstash 呢，其实是因为在测试环境上面腾讯云 tke 的日志没有开启日志收集，所以在排查问题的时候会十分的痛苦，正好有空了就想着将日志抽出来放进 es 里面，方便以后排查问题，正好看到腾讯云的日志规则是允许将 pod 的 stdout 日志进行采集之后投递到 kafka 的，就小试了一下。
部署 logstash logstash 我选择使用 docker-compose 来进行快速的部署。
以下是部署流程，参考自 deviantony/docker-elk 项目
创建目录 mkdir logstash/config logstash/pipeline -p 创建环境变量 路径 .env
ELASTIC_VERSION=8.7.1 LOGSTASH_INTERNAL_PASSWORD='changeme' 创建 Dockerfile 路径 logstasg/Dockerfile
ARG ELASTIC_VERSION # https://www.docker.elastic.co/ FROM docker.elastic.co/logstash/logstash:${ELASTIC_VERSION} 配置文件 路径 logstash/config/logstash.yml
--- ## Default Logstash configuration from Logstash base image. ## https://github.com/elastic/logstash/blob/main/docker/data/logstash/config/logstash-full.yml # http.host: 0.0.0.0 node.name: logstash 路径 logstash/pipeline/logstash.conf
input { beats { port => 5044 } tcp { port => 50000 } } ## Add your filters / logstash plugins configuration here output { elasticsearch { hosts => &#34;elasticsearch:9200&#34; user => &#34;logstash_internal&#34; password => &#34;${LOGSTASH_INTERNAL_PASSWORD}&#34; index => &#34;logstash-%{+YYYY-MM-dd}&#34; } } 启动服务 version: '3."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://blog.lvcshu.com/posts/"},{"@type":"ListItem","position":2,"name":"使用 logstash 采集来自腾讯云 tke 的日志","item":"https://blog.lvcshu.com/2023/05/15/%E4%BD%BF%E7%94%A8-logstash-%E9%87%87%E9%9B%86%E6%9D%A5%E8%87%AA%E8%85%BE%E8%AE%AF%E4%BA%91-tke-%E7%9A%84%E6%97%A5%E5%BF%97/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"使用 logstash 采集来自腾讯云 tke 的日志","name":"使用 logstash 采集来自腾讯云 tke 的日志","description":"前提 好久没有给博客除草了，正好最近折腾了下 logstash，记录一下。\n为啥要用 logstash 呢，其实是因为在测试环境上面腾讯云 tke 的日志没有开启日志收集，所以在排查问题的时候会十分的痛苦，正好有空了就想着将日志抽出来放进 es 里面，方便以后排查问题，正好看到腾讯云的日志规则是允许将 pod 的 stdout 日志进行采集之后投递到 kafka 的，就小试了一下。\n部署 logstash logstash 我选择使用 docker-compose 来进行快速的部署。\n以下是部署流程，参考自 deviantony/docker-elk 项目\n创建目录 mkdir logstash/config logstash/pipeline -p 创建环境变量 路径 .env\nELASTIC_VERSION=8.7.1 LOGSTASH_INTERNAL_PASSWORD=\u0026#39;changeme\u0026#39; 创建 Dockerfile 路径 logstasg/Dockerfile\nARG ELASTIC_VERSION # https://www.docker.elastic.co/ FROM docker.elastic.co/logstash/logstash:${ELASTIC_VERSION} 配置文件 路径 logstash/config/logstash.yml\n--- ## Default Logstash configuration from Logstash base image. ## https://github.com/elastic/logstash/blob/main/docker/data/logstash/config/logstash-full.yml # http.host: 0.0.0.0 node.name: logstash 路径 logstash/pipeline/logstash.conf\ninput { beats { port =\u0026gt; 5044 } tcp { port =\u0026gt; 50000 } } ## Add your filters / logstash plugins configuration here output { elasticsearch { hosts =\u0026gt; \u0026#34;elasticsearch:9200\u0026#34; user =\u0026gt; \u0026#34;logstash_internal\u0026#34; password =\u0026gt; \u0026#34;${LOGSTASH_INTERNAL_PASSWORD}\u0026#34; index =\u0026gt; \u0026#34;logstash-%{+YYYY-MM-dd}\u0026#34; } } 启动服务 version: \u0026#39;3.","keywords":["logstash","腾讯云","笔记"],"articleBody":"前提 好久没有给博客除草了，正好最近折腾了下 logstash，记录一下。\n为啥要用 logstash 呢，其实是因为在测试环境上面腾讯云 tke 的日志没有开启日志收集，所以在排查问题的时候会十分的痛苦，正好有空了就想着将日志抽出来放进 es 里面，方便以后排查问题，正好看到腾讯云的日志规则是允许将 pod 的 stdout 日志进行采集之后投递到 kafka 的，就小试了一下。\n部署 logstash logstash 我选择使用 docker-compose 来进行快速的部署。\n以下是部署流程，参考自 deviantony/docker-elk 项目\n创建目录 mkdir logstash/config logstash/pipeline -p 创建环境变量 路径 .env\nELASTIC_VERSION=8.7.1 LOGSTASH_INTERNAL_PASSWORD='changeme' 创建 Dockerfile 路径 logstasg/Dockerfile\nARG ELASTIC_VERSION # https://www.docker.elastic.co/ FROM docker.elastic.co/logstash/logstash:${ELASTIC_VERSION} 配置文件 路径 logstash/config/logstash.yml\n--- ## Default Logstash configuration from Logstash base image. ## https://github.com/elastic/logstash/blob/main/docker/data/logstash/config/logstash-full.yml # http.host: 0.0.0.0 node.name: logstash 路径 logstash/pipeline/logstash.conf\ninput { beats { port =\u003e 5044 } tcp { port =\u003e 50000 } } ## Add your filters / logstash plugins configuration here output { elasticsearch { hosts =\u003e \"elasticsearch:9200\" user =\u003e \"logstash_internal\" password =\u003e \"${LOGSTASH_INTERNAL_PASSWORD}\" index =\u003e \"logstash-%{+YYYY-MM-dd}\" } } 启动服务 version: '3.7' services: logstash: build: context: logstash/ args: ELASTIC_VERSION: ${ELASTIC_VERSION} volumes: - ./logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml:ro,Z - ./logstash/pipeline:/usr/share/logstash/pipeline:ro,Z ports: - 5044:5044 - 50000:50000/tcp - 50000:50000/udp - 9600:9600 environment: LS_JAVA_OPTS: -Xms256m -Xmx256m LOGSTASH_INTERNAL_PASSWORD: ${LOGSTASH_INTERNAL_PASSWORD:-} depends_on: - elasticsearch restart: unless-stopped 配置 logstash pipeline 配置 input 因为日志要从 kafka 读取，所以要在 input 块内声明一个新的数据源\ninput { beats { port =\u003e 5044 } tcp { port =\u003e 50000 } kafka { bootstrap_servers =\u003e \"kafka 地址\" client_id =\u003e \"test_logstash\" group_id =\u003e \"test_logstash\" auto_offset_reset =\u003e \"latest\" consumer_threads =\u003e 1 decorate_events =\u003e true topics =\u003e [\"tencent-tke-log\"] type =\u003e \"bhy\" } } 配置过滤器 由于腾讯云投递到kafka的数据不是可以直接用的，数据示例如下\n{\"@timestamp\":1684203000.007603,\"@filepath\":\"/var/log/tke-log-agent/log/stdout-containerd/xxxxxxx.log\",\"log\":\"2023-05-16T10:10:00.002817673+08:00 stdout F {\\\"level\\\":\\\"debug\\\",\\\"time\\\":\\\"2023-05-16T10:10:00.002+0800\\\",\\\"msg\\\":\\\"xxxxxxxx\\\"}\",\"kubernetes\":{\"pod_name\":\"xxxxx\",\"namespace_name\":\"default\",\"pod_id\":\"xxxxxx\",\"labels\":{\"k8s-app\":\"xxxxx\",\"pod-template-hash\":\"xxxxx\",\"qcloud-app\":\"xxxxxxxx\"},\"annotations\":{\"qcloud-redeploy-timestamp\":\"1681975158658\",\"tke.cloud.tencent.com/networks-status\":\"json\"},\"host\":\"xxxxx\",\"container_name\":\"xxxx\",\"docker_id\":\"xxxxx\",\"container_hash\":\"xxxxxx\",\"container_image\":\"xxxxxxxx\"}} 这时候就要对数据进行预处理，因为有很多数据是不需要添加进入es里面的。使用了 logstags 的 filter 进行处理。\n保存原始数据 这个过滤器添加了一个叫做 source 的字段，并且将 kafka 消息全部存储在里面\nmutate { add_field =\u003e { \"source\" =\u003e \"%{[message]}\" } } 解析 json 使用 json 插件，对 message 字段的 json 字符串进行序列化，并且将序列化后的数据存储在 kafkajson 字段内，同时删除 message 字段\njson { source =\u003e \"message\" remove_field =\u003e [ \"message\" ] target =\u003e \"kafkajson\" } 提取日志元数据 将日志中的一些定位数据提取到外层\nmutate { rename =\u003e { \"[kafkajson][kubernetes][namespace_name]\" =\u003e \"namespace\" } rename =\u003e { \"[kafkajson][kubernetes][pod_name]\" =\u003e \"podname\" } rename =\u003e { \"[kafkajson][kubernetes][labels][k8s-app]\" =\u003e \"k8s-app\" } rename =\u003e { \"[kafkajson][kubernetes][container_image]\" =\u003e \"container-image\" } } 使用正则表达式处理数据 腾讯云给出的数据会在服务自己打印的日志前方添加时间以及 ”stdout F“，所以要使用正则表达式将我们不需要的东西做下替换\n2023-05-16T10:10:00.002817673+08:00 stdout F {\\\"level\\\":\\\"debug\\\",\\\"time\\\":\\\"2023-05-16T10:10:00.002+0800\\\",\\\"msg\\\":\\\"xxxxx\\\"} mutate { gsub =\u003e [ \"[kafkajson][log]\",\".+stdout\\sF\\s\",\"\" ] } 提取服务打印的日志 这回终于可以将日志提取出来了\njson { source =\u003e \"[kafkajson][log]\" remove_field =\u003e [ \"kafkajson\" ] target =\u003e \"data\" } 提取日志等级 最后一步，将日志的等级字段提取到外层，方便使用\nmutate { rename =\u003e { \"[data][level]\" =\u003e \"level\" } } 完整 pipeline input { beats { port =\u003e 5044 } tcp { port =\u003e 50000 } kafka{ bootstrap_servers =\u003e \"kafka 地址\" client_id =\u003e \"test_logstash\" group_id =\u003e \"test_logstash\" auto_offset_reset =\u003e \"latest\" consumer_threads =\u003e 1 decorate_events =\u003e true topics =\u003e [\"tencent-tke-log\"] type =\u003e \"bhy\" } } filter { mutate { add_field =\u003e { \"source\" =\u003e \"%{[message]}\" } } json { source =\u003e \"message\" remove_field =\u003e [ \"message\" ] target =\u003e \"kafkajson\" } mutate { rename =\u003e { \"[kafkajson][kubernetes][namespace_name]\" =\u003e \"namespace\" } rename =\u003e { \"[kafkajson][kubernetes][pod_name]\" =\u003e \"podname\" } rename =\u003e { \"[kafkajson][kubernetes][labels][k8s-app]\" =\u003e \"k8s-app\" } rename =\u003e { \"[kafkajson][kubernetes][container_image]\" =\u003e \"container-image\" } } mutate { gsub =\u003e [ \"[kafkajson][log]\",\".+stdout\\sF\\s\",\"\" ] } json { source =\u003e \"[kafkajson][log]\" remove_field =\u003e [ \"kafkajson\" ] target =\u003e \"data\" } mutate { rename =\u003e { \"[data][level]\" =\u003e \"level\" } } } ## Add your filters / logstash plugins configuration here output { elasticsearch { hosts =\u003e \"elasticsearch:9200\" user =\u003e \"logstash_internal\" password =\u003e \"${LOGSTASH_INTERNAL_PASSWORD}\" index =\u003e \"logstash-%{+YYYY-MM-dd}\" } } 特别鸣谢 在本次配置 logstash 的过程中，我自己搭建的、使用 openAPI API 的、基于 GPT3.5 语言模型的 telegram bot 给予了我很大的帮助。\n以下是我与它的对话(对话内容经过处理)\njohnpoint, [5/15/23 2:18 PM] 我想从 2023-05-15T13:51:00.004687384+08:00 stdout F {\"level\":\"debug\",\"time\":\"2023-05-15T13:51:00.002+0800\",\"msg\":\"xxxxxxxx\"} 这些数据里面使用logstash 提取出json数据，logstash 的pipeline该怎么写 openAI, [5/15/23 2:18 PM] 假设你的日志文件是以JSON格式输出的，你需要配置一个JSON解析器来解析日志中的JSON数据，并将其转换为可用的事件。以下是一个简单的Logstash配置文件： ``` input { file { path =\u003e \"/path/to/your/logfile\" start_position =\u003e \"beginning\" sincedb_path =\u003e \"/dev/null\" } } filter { json { source =\u003e \"message\" } } output { stdout { codec =\u003e rubydebug } } ``` 在logstash的input部分，我们指定读取的文件的路径，以及从文件的开始位置开始读入数据。 在filter部分，我们使用`json`插件从日志中提取json对象。 最后，我们在output部分使用`rubydebug`进行输出。 运行这个Logstash的配置命令： ``` bin/logstash -f /path/to/your/config/file ``` 这样，logstash就会从你指定的文件中读取数据，并把提取的JSON对象输出到控制台上。 johnpoint, [5/15/23 2:19 PM] 这个数据有json格式的数据，但是在json格式的数据前面还有一些脏数据，是否可以去除？ openAI, [5/15/23 2:19 PM] 可以，在使用 logstash 分析数据的时候，可以使用 grok 这个插件，通过正则表达式匹配出需要的数据。 以下是一个可能符合你提供的数据的 grok 表达式： ``` %{TIMESTAMP_ISO8601:timestamp} stdout F %{GREEDYDATA:json_data} ``` 这个正则表达式的含义是匹配以 ISO8601 格式表示的时间戳（如 \"2023-05-15T13:51:00.004687384+08:00\"）和以字符串 \"stdout F \" 结尾的内容，然后跟着一个 Json 格式的字符串。\"%{GREEDYDATA:json_data}\" 表示将这个字符串保存到一个名为 \"json_data\" 的字段中。 上述 grok 表达式可以在 Logstash 的配置文件中使用，例如： ``` input { # 配置 logstash 接收数据的源 } filter { grok { match =\u003e { \"message\" =\u003e \"%{TIMESTAMP_ISO8601:timestamp} stdout F %{GREEDYDATA:json_data}\" } } # 其他过滤器 } output { # 配置 logstash 输出数据的目标 } ``` 这个配置文件中，通过 filter 下的 grok 插件匹配出了 timestamp 和 json_data 两个字段。从而可以使用 logstash 利用这些数据进行更进一步的操作，如输出到 Elasticsearch 或者 stdout。达到你想要的结果。 其中 `%{TIMESTAMP_ISO8601:timestamp}` 表示匹配 ISO8601 格式的时间戳，并将其保存到一个名为 \"timestamp\" 的字段中。`%{GREEDYDATA:json_data}` 表示匹配任意字符，可以匹配行末附加的不符合 Json 格式的内容。因为 grok 会一直匹配到行尾，所以匹配过程中会包括最后一个字段（即 json_data）中包含的所有内容。 johnpoint, [5/15/23 2:36 PM] 返回 Can't get text on a START_OBJECT at 1:8 是什么错误 openAI, [5/15/23 2:36 PM] 这个错误常见于尝试将 JSON 对象作为字符串进行解析的情况下，但是实际上得到的是 JSON 对象，而不是字符串。这意味着该对象的开头是一个“{”，而不是一个“\"”。可以检查代码调用的函数或方法，以确保解析的字符串确实是一个字符串。另外，可以检查 JSON 对象的源代码，以确保它是按照预期格式进行编写的。 johnpoint, [5/15/23 2:53 PM] [2023-05-15T06:52:48,511][WARN ][logstash.filters.json ][main][xxxxxx] Error parsing json {:source=\u003e\"json_data\", :raw=\u003e\"{\\\\\\\"level\\\\\\\":\\\\\\\"debug\\\\\\\",\\\\\\\"time\\\\\\\":\\\\\\\"2023-05-15T14:52:47.664+0800\\\\\\\",\\\\\\\"msg\\\\\\\":\\\\\\\"xxxxxxxx\\\\\\\"}\\\", \\\"@timestamp\\\"=\u003e0.1684133567674155e10}\", :exception=\u003e#","wordCount":"1200","inLanguage":"en","datePublished":"2023-05-15T10:34:44Z","dateModified":"2023-05-15T10:34:44Z","author":{"@type":"Person","name":"johnpoint"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.lvcshu.com/2023/05/15/%E4%BD%BF%E7%94%A8-logstash-%E9%87%87%E9%9B%86%E6%9D%A5%E8%87%AA%E8%85%BE%E8%AE%AF%E4%BA%91-tke-%E7%9A%84%E6%97%A5%E5%BF%97/"},"publisher":{"@type":"Organization","name":"johnpoint's blog","logo":{"@type":"ImageObject","url":"https://blog.lvcshu.com/favicon.ico"}}}</script></head><body class=dark id=top><header class=header><nav class=nav><div class=logo><a href=https://blog.lvcshu.com/ accesskey=h title="johnpoint's blog (Alt + H)">johnpoint's blog</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://blog.lvcshu.com/posts/ title=归档><span>归档</span></a></li><li><a href=https://blog.lvcshu.com/friends/ title=友链><span>友链</span></a></li><li><a href=https://blog.lvcshu.com/about/ title=关于我><span>关于我</span></a></li><li><a href=https://blog.lvcshu.com/search/ title="搜索 (Alt + /)" accesskey=/><span>搜索</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>使用 logstash 采集来自腾讯云 tke 的日志</h1><div class=post-meta><span title='2023-05-15 10:34:44 +0000 UTC'>2023-05-15 10:34:44</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;1200 words&nbsp;·&nbsp;johnpoint</div></header><div class=post-content><h2 id=前提>前提<a hidden class=anchor aria-hidden=true href=#前提>#</a></h2><p>好久没有给博客除草了，正好最近折腾了下 logstash，记录一下。</p><p>为啥要用 logstash 呢，其实是因为在测试环境上面腾讯云 tke 的日志没有开启日志收集，所以在排查问题的时候会十分的痛苦，正好有空了就想着将日志抽出来放进 es 里面，方便以后排查问题，正好看到腾讯云的日志规则是允许将 pod 的 stdout 日志进行采集之后投递到 kafka 的，就小试了一下。</p><h2 id=部署-logstash>部署 logstash<a hidden class=anchor aria-hidden=true href=#部署-logstash>#</a></h2><p>logstash 我选择使用 docker-compose 来进行快速的部署。</p><p>以下是部署流程，参考自 <a href=https://github.com/deviantony/docker-elk>deviantony/docker-elk</a> 项目</p><h3 id=创建目录>创建目录<a hidden class=anchor aria-hidden=true href=#创建目录>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>mkdir logstash/config logstash/pipeline -p
</span></span></code></pre></div><h3 id=创建环境变量>创建环境变量<a hidden class=anchor aria-hidden=true href=#创建环境变量>#</a></h3><p>路径 <code>.env</code></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-ini data-lang=ini><span class=line><span class=cl><span class=na>ELASTIC_VERSION</span><span class=o>=</span><span class=s>8.7.1</span>
</span></span><span class=line><span class=cl><span class=na>LOGSTASH_INTERNAL_PASSWORD</span><span class=o>=</span><span class=s>&#39;changeme&#39;</span>
</span></span></code></pre></div><h3 id=创建-dockerfile>创建 Dockerfile<a hidden class=anchor aria-hidden=true href=#创建-dockerfile>#</a></h3><p>路径 <code>logstasg/Dockerfile</code></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-Dockerfile data-lang=Dockerfile><span class=line><span class=cl><span class=k>ARG</span> ELASTIC_VERSION<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># https://www.docker.elastic.co/</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>FROM</span><span class=s> docker.elastic.co/logstash/logstash:${ELASTIC_VERSION}</span><span class=err>
</span></span></span></code></pre></div><h3 id=配置文件>配置文件<a hidden class=anchor aria-hidden=true href=#配置文件>#</a></h3><p>路径 <code>logstash/config/logstash.yml</code></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yml data-lang=yml><span class=line><span class=cl><span class=nn>---</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c>## Default Logstash configuration from Logstash base image.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c>## https://github.com/elastic/logstash/blob/main/docker/data/logstash/config/logstash-full.yml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c>#</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>http.host</span><span class=p>:</span><span class=w> </span><span class=m>0.0.0.0</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>node.name</span><span class=p>:</span><span class=w> </span><span class=l>logstash</span><span class=w>
</span></span></span></code></pre></div><p>路径 <code>logstash/pipeline/logstash.conf</code></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-ruby data-lang=ruby><span class=line><span class=cl><span class=n>input</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=n>beats</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>		<span class=n>port</span> <span class=o>=&gt;</span> <span class=mi>5044</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=n>tcp</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>		<span class=n>port</span> <span class=o>=&gt;</span> <span class=mi>50000</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>## Add your filters / logstash plugins configuration here</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=n>elasticsearch</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>		<span class=n>hosts</span> <span class=o>=&gt;</span> <span class=s2>&#34;elasticsearch:9200&#34;</span>
</span></span><span class=line><span class=cl>		<span class=n>user</span> <span class=o>=&gt;</span> <span class=s2>&#34;logstash_internal&#34;</span>
</span></span><span class=line><span class=cl>		<span class=n>password</span> <span class=o>=&gt;</span> <span class=s2>&#34;${LOGSTASH_INTERNAL_PASSWORD}&#34;</span>
</span></span><span class=line><span class=cl>		<span class=n>index</span> <span class=o>=&gt;</span> <span class=s2>&#34;logstash-%{+YYYY-MM-dd}&#34;</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><h3 id=启动服务>启动服务<a hidden class=anchor aria-hidden=true href=#启动服务>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>version</span><span class=p>:</span><span class=w> </span><span class=s1>&#39;3.7&#39;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>services</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>logstash</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>build</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>context</span><span class=p>:</span><span class=w> </span><span class=l>logstash/</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>args</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>ELASTIC_VERSION</span><span class=p>:</span><span class=w> </span><span class=l>${ELASTIC_VERSION}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>./logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml:ro,Z</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>./logstash/pipeline:/usr/share/logstash/pipeline:ro,Z</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=m>5044</span><span class=p>:</span><span class=m>5044</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=m>50000</span><span class=p>:</span><span class=m>50000</span><span class=l>/tcp</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=m>50000</span><span class=p>:</span><span class=m>50000</span><span class=l>/udp</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=m>9600</span><span class=p>:</span><span class=m>9600</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>environment</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>LS_JAVA_OPTS</span><span class=p>:</span><span class=w> </span>-<span class=l>Xms256m -Xmx256m</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>LOGSTASH_INTERNAL_PASSWORD</span><span class=p>:</span><span class=w> </span><span class=l>${LOGSTASH_INTERNAL_PASSWORD:-}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>depends_on</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>elasticsearch</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>restart</span><span class=p>:</span><span class=w> </span><span class=l>unless-stopped</span><span class=w>
</span></span></span></code></pre></div><h2 id=配置-logstash-pipeline>配置 logstash pipeline<a hidden class=anchor aria-hidden=true href=#配置-logstash-pipeline>#</a></h2><h3 id=配置-input>配置 input<a hidden class=anchor aria-hidden=true href=#配置-input>#</a></h3><p>因为日志要从 kafka 读取，所以要在 input 块内声明一个新的数据源</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-ruby data-lang=ruby><span class=line><span class=cl><span class=n>input</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=n>beats</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>		<span class=n>port</span> <span class=o>=&gt;</span> <span class=mi>5044</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=n>tcp</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>		<span class=n>port</span> <span class=o>=&gt;</span> <span class=mi>50000</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=n>kafka</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>		<span class=n>bootstrap_servers</span> <span class=o>=&gt;</span> <span class=s2>&#34;kafka 地址&#34;</span>
</span></span><span class=line><span class=cl>		<span class=n>client_id</span> <span class=o>=&gt;</span> <span class=s2>&#34;test_logstash&#34;</span>
</span></span><span class=line><span class=cl>		<span class=n>group_id</span> <span class=o>=&gt;</span> <span class=s2>&#34;test_logstash&#34;</span>
</span></span><span class=line><span class=cl>		<span class=n>auto_offset_reset</span> <span class=o>=&gt;</span> <span class=s2>&#34;latest&#34;</span> 
</span></span><span class=line><span class=cl>		<span class=n>consumer_threads</span> <span class=o>=&gt;</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>		<span class=n>decorate_events</span> <span class=o>=&gt;</span> <span class=kp>true</span> 
</span></span><span class=line><span class=cl>		<span class=n>topics</span> <span class=o>=&gt;</span> <span class=o>[</span><span class=s2>&#34;tencent-tke-log&#34;</span><span class=o>]</span>
</span></span><span class=line><span class=cl>		<span class=n>type</span> <span class=o>=&gt;</span> <span class=s2>&#34;bhy&#34;</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><h3 id=配置过滤器>配置过滤器<a hidden class=anchor aria-hidden=true href=#配置过滤器>#</a></h3><p>由于腾讯云投递到kafka的数据不是可以直接用的，数据示例如下</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span><span class=nt>&#34;@timestamp&#34;</span><span class=p>:</span><span class=mf>1684203000.007603</span><span class=p>,</span><span class=nt>&#34;@filepath&#34;</span><span class=p>:</span><span class=s2>&#34;/var/log/tke-log-agent/log/stdout-containerd/xxxxxxx.log&#34;</span><span class=p>,</span><span class=nt>&#34;log&#34;</span><span class=p>:</span><span class=s2>&#34;2023-05-16T10:10:00.002817673+08:00 stdout F {\&#34;level\&#34;:\&#34;debug\&#34;,\&#34;time\&#34;:\&#34;2023-05-16T10:10:00.002+0800\&#34;,\&#34;msg\&#34;:\&#34;xxxxxxxx\&#34;}&#34;</span><span class=p>,</span><span class=nt>&#34;kubernetes&#34;</span><span class=p>:{</span><span class=nt>&#34;pod_name&#34;</span><span class=p>:</span><span class=s2>&#34;xxxxx&#34;</span><span class=p>,</span><span class=nt>&#34;namespace_name&#34;</span><span class=p>:</span><span class=s2>&#34;default&#34;</span><span class=p>,</span><span class=nt>&#34;pod_id&#34;</span><span class=p>:</span><span class=s2>&#34;xxxxxx&#34;</span><span class=p>,</span><span class=nt>&#34;labels&#34;</span><span class=p>:{</span><span class=nt>&#34;k8s-app&#34;</span><span class=p>:</span><span class=s2>&#34;xxxxx&#34;</span><span class=p>,</span><span class=nt>&#34;pod-template-hash&#34;</span><span class=p>:</span><span class=s2>&#34;xxxxx&#34;</span><span class=p>,</span><span class=nt>&#34;qcloud-app&#34;</span><span class=p>:</span><span class=s2>&#34;xxxxxxxx&#34;</span><span class=p>},</span><span class=nt>&#34;annotations&#34;</span><span class=p>:{</span><span class=nt>&#34;qcloud-redeploy-timestamp&#34;</span><span class=p>:</span><span class=s2>&#34;1681975158658&#34;</span><span class=p>,</span><span class=nt>&#34;tke.cloud.tencent.com/networks-status&#34;</span><span class=p>:</span><span class=s2>&#34;json&#34;</span><span class=p>},</span><span class=nt>&#34;host&#34;</span><span class=p>:</span><span class=s2>&#34;xxxxx&#34;</span><span class=p>,</span><span class=nt>&#34;container_name&#34;</span><span class=p>:</span><span class=s2>&#34;xxxx&#34;</span><span class=p>,</span><span class=nt>&#34;docker_id&#34;</span><span class=p>:</span><span class=s2>&#34;xxxxx&#34;</span><span class=p>,</span><span class=nt>&#34;container_hash&#34;</span><span class=p>:</span><span class=s2>&#34;xxxxxx&#34;</span><span class=p>,</span><span class=nt>&#34;container_image&#34;</span><span class=p>:</span><span class=s2>&#34;xxxxxxxx&#34;</span><span class=p>}}</span>
</span></span></code></pre></div><p>这时候就要对数据进行预处理，因为有很多数据是不需要添加进入es里面的。使用了 logstags 的 filter 进行处理。</p><h4 id=保存原始数据>保存原始数据<a hidden class=anchor aria-hidden=true href=#保存原始数据>#</a></h4><p>这个过滤器添加了一个叫做 source 的字段，并且将 kafka 消息全部存储在里面</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-ruby data-lang=ruby><span class=line><span class=cl>	<span class=n>mutate</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>		<span class=n>add_field</span> <span class=o>=&gt;</span> <span class=p>{</span> <span class=s2>&#34;source&#34;</span> <span class=o>=&gt;</span> <span class=s2>&#34;%{[message]}&#34;</span> <span class=p>}</span>
</span></span><span class=line><span class=cl>  	<span class=p>}</span>
</span></span></code></pre></div><h4 id=解析-json>解析 json<a hidden class=anchor aria-hidden=true href=#解析-json>#</a></h4><p>使用 json 插件，对 message 字段的 json 字符串进行序列化，并且将序列化后的数据存储在 kafkajson 字段内，同时删除 message 字段</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-ruby data-lang=ruby><span class=line><span class=cl>	<span class=n>json</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>		<span class=n>source</span> <span class=o>=&gt;</span> <span class=s2>&#34;message&#34;</span>
</span></span><span class=line><span class=cl>		<span class=n>remove_field</span> <span class=o>=&gt;</span> <span class=o>[</span> 
</span></span><span class=line><span class=cl>			<span class=s2>&#34;message&#34;</span>
</span></span><span class=line><span class=cl>		<span class=o>]</span>
</span></span><span class=line><span class=cl>		<span class=n>target</span> <span class=o>=&gt;</span> <span class=s2>&#34;kafkajson&#34;</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span></code></pre></div><h4 id=提取日志元数据>提取日志元数据<a hidden class=anchor aria-hidden=true href=#提取日志元数据>#</a></h4><p>将日志中的一些定位数据提取到外层</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-ruby data-lang=ruby><span class=line><span class=cl>	<span class=n>mutate</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    	<span class=n>rename</span> <span class=o>=&gt;</span> <span class=p>{</span> <span class=s2>&#34;[kafkajson][kubernetes][namespace_name]&#34;</span> <span class=o>=&gt;</span> <span class=s2>&#34;namespace&#34;</span> <span class=p>}</span>
</span></span><span class=line><span class=cl>    	<span class=n>rename</span> <span class=o>=&gt;</span> <span class=p>{</span> <span class=s2>&#34;[kafkajson][kubernetes][pod_name]&#34;</span> <span class=o>=&gt;</span> <span class=s2>&#34;podname&#34;</span> <span class=p>}</span>
</span></span><span class=line><span class=cl>    	<span class=n>rename</span> <span class=o>=&gt;</span> <span class=p>{</span> <span class=s2>&#34;[kafkajson][kubernetes][labels][k8s-app]&#34;</span> <span class=o>=&gt;</span> <span class=s2>&#34;k8s-app&#34;</span> <span class=p>}</span>
</span></span><span class=line><span class=cl>    	<span class=n>rename</span> <span class=o>=&gt;</span> <span class=p>{</span> <span class=s2>&#34;[kafkajson][kubernetes][container_image]&#34;</span> <span class=o>=&gt;</span> <span class=s2>&#34;container-image&#34;</span> <span class=p>}</span>
</span></span><span class=line><span class=cl>  	<span class=p>}</span>
</span></span></code></pre></div><h4 id=使用正则表达式处理数据>使用正则表达式处理数据<a hidden class=anchor aria-hidden=true href=#使用正则表达式处理数据>#</a></h4><p>腾讯云给出的数据会在服务自己打印的日志前方添加时间以及 ”stdout F“，所以要使用正则表达式将我们不需要的东西做下替换</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>2023-05-16T10:10:00.002817673+08:00 stdout F <span class=o>{</span><span class=se>\&#34;</span>level<span class=se>\&#34;</span>:<span class=se>\&#34;</span>debug<span class=se>\&#34;</span>,<span class=se>\&#34;</span>time<span class=se>\&#34;</span>:<span class=se>\&#34;</span>2023-05-16T10:10:00.002+0800<span class=se>\&#34;</span>,<span class=se>\&#34;</span>msg<span class=se>\&#34;</span>:<span class=se>\&#34;</span>xxxxx<span class=se>\&#34;</span><span class=o>}</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-ruby data-lang=ruby><span class=line><span class=cl>	<span class=n>mutate</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>		<span class=nb>gsub</span> <span class=o>=&gt;</span> <span class=o>[</span> <span class=s2>&#34;[kafkajson][log]&#34;</span><span class=p>,</span><span class=s2>&#34;.+stdout</span><span class=se>\s</span><span class=s2>F</span><span class=se>\s</span><span class=s2>&#34;</span><span class=p>,</span><span class=s2>&#34;&#34;</span> <span class=o>]</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span></code></pre></div><h4 id=提取服务打印的日志>提取服务打印的日志<a hidden class=anchor aria-hidden=true href=#提取服务打印的日志>#</a></h4><p>这回终于可以将日志提取出来了</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-ruby data-lang=ruby><span class=line><span class=cl>	<span class=n>json</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>		<span class=n>source</span> <span class=o>=&gt;</span> <span class=s2>&#34;[kafkajson][log]&#34;</span>
</span></span><span class=line><span class=cl>		<span class=n>remove_field</span> <span class=o>=&gt;</span> <span class=o>[</span> 
</span></span><span class=line><span class=cl>			<span class=s2>&#34;kafkajson&#34;</span>
</span></span><span class=line><span class=cl>		<span class=o>]</span>
</span></span><span class=line><span class=cl>		<span class=n>target</span> <span class=o>=&gt;</span> <span class=s2>&#34;data&#34;</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span></code></pre></div><h4 id=提取日志等级>提取日志等级<a hidden class=anchor aria-hidden=true href=#提取日志等级>#</a></h4><p>最后一步，将日志的等级字段提取到外层，方便使用</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-ruby data-lang=ruby><span class=line><span class=cl>	<span class=n>mutate</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    	<span class=n>rename</span> <span class=o>=&gt;</span> <span class=p>{</span> <span class=s2>&#34;[data][level]&#34;</span> <span class=o>=&gt;</span> <span class=s2>&#34;level&#34;</span> <span class=p>}</span>
</span></span><span class=line><span class=cl>  	<span class=p>}</span>
</span></span></code></pre></div><h3 id=完整-pipeline>完整 pipeline<a hidden class=anchor aria-hidden=true href=#完整-pipeline>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-ruby data-lang=ruby><span class=line><span class=cl><span class=n>input</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=n>beats</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>		<span class=n>port</span> <span class=o>=&gt;</span> <span class=mi>5044</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=n>tcp</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>		<span class=n>port</span> <span class=o>=&gt;</span> <span class=mi>50000</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=n>kafka</span><span class=p>{</span>
</span></span><span class=line><span class=cl>		<span class=n>bootstrap_servers</span> <span class=o>=&gt;</span> <span class=s2>&#34;kafka 地址&#34;</span>
</span></span><span class=line><span class=cl>		<span class=n>client_id</span> <span class=o>=&gt;</span> <span class=s2>&#34;test_logstash&#34;</span>
</span></span><span class=line><span class=cl>		<span class=n>group_id</span> <span class=o>=&gt;</span> <span class=s2>&#34;test_logstash&#34;</span>
</span></span><span class=line><span class=cl>		<span class=n>auto_offset_reset</span> <span class=o>=&gt;</span> <span class=s2>&#34;latest&#34;</span> 
</span></span><span class=line><span class=cl>		<span class=n>consumer_threads</span> <span class=o>=&gt;</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>		<span class=n>decorate_events</span> <span class=o>=&gt;</span> <span class=kp>true</span> 
</span></span><span class=line><span class=cl>		<span class=n>topics</span> <span class=o>=&gt;</span> <span class=o>[</span><span class=s2>&#34;tencent-tke-log&#34;</span><span class=o>]</span>
</span></span><span class=line><span class=cl>		<span class=n>type</span> <span class=o>=&gt;</span> <span class=s2>&#34;bhy&#34;</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>filter</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=n>mutate</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>		<span class=n>add_field</span> <span class=o>=&gt;</span> <span class=p>{</span> <span class=s2>&#34;source&#34;</span> <span class=o>=&gt;</span> <span class=s2>&#34;%{[message]}&#34;</span> <span class=p>}</span>
</span></span><span class=line><span class=cl>  	<span class=p>}</span>
</span></span><span class=line><span class=cl>	<span class=n>json</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>		<span class=n>source</span> <span class=o>=&gt;</span> <span class=s2>&#34;message&#34;</span>
</span></span><span class=line><span class=cl>		<span class=n>remove_field</span> <span class=o>=&gt;</span> <span class=o>[</span> 
</span></span><span class=line><span class=cl>			<span class=s2>&#34;message&#34;</span>
</span></span><span class=line><span class=cl>		<span class=o>]</span>
</span></span><span class=line><span class=cl>		<span class=n>target</span> <span class=o>=&gt;</span> <span class=s2>&#34;kafkajson&#34;</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span><span class=line><span class=cl>	<span class=n>mutate</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    	<span class=n>rename</span> <span class=o>=&gt;</span> <span class=p>{</span> <span class=s2>&#34;[kafkajson][kubernetes][namespace_name]&#34;</span> <span class=o>=&gt;</span> <span class=s2>&#34;namespace&#34;</span> <span class=p>}</span>
</span></span><span class=line><span class=cl>    	<span class=n>rename</span> <span class=o>=&gt;</span> <span class=p>{</span> <span class=s2>&#34;[kafkajson][kubernetes][pod_name]&#34;</span> <span class=o>=&gt;</span> <span class=s2>&#34;podname&#34;</span> <span class=p>}</span>
</span></span><span class=line><span class=cl>    	<span class=n>rename</span> <span class=o>=&gt;</span> <span class=p>{</span> <span class=s2>&#34;[kafkajson][kubernetes][labels][k8s-app]&#34;</span> <span class=o>=&gt;</span> <span class=s2>&#34;k8s-app&#34;</span> <span class=p>}</span>
</span></span><span class=line><span class=cl>    	<span class=n>rename</span> <span class=o>=&gt;</span> <span class=p>{</span> <span class=s2>&#34;[kafkajson][kubernetes][container_image]&#34;</span> <span class=o>=&gt;</span> <span class=s2>&#34;container-image&#34;</span> <span class=p>}</span>
</span></span><span class=line><span class=cl>  	<span class=p>}</span>
</span></span><span class=line><span class=cl>	<span class=n>mutate</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>		<span class=nb>gsub</span> <span class=o>=&gt;</span> <span class=o>[</span> <span class=s2>&#34;[kafkajson][log]&#34;</span><span class=p>,</span><span class=s2>&#34;.+stdout</span><span class=se>\s</span><span class=s2>F</span><span class=se>\s</span><span class=s2>&#34;</span><span class=p>,</span><span class=s2>&#34;&#34;</span> <span class=o>]</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span><span class=line><span class=cl>	<span class=n>json</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>		<span class=n>source</span> <span class=o>=&gt;</span> <span class=s2>&#34;[kafkajson][log]&#34;</span>
</span></span><span class=line><span class=cl>		<span class=n>remove_field</span> <span class=o>=&gt;</span> <span class=o>[</span> 
</span></span><span class=line><span class=cl>			<span class=s2>&#34;kafkajson&#34;</span>
</span></span><span class=line><span class=cl>		<span class=o>]</span>
</span></span><span class=line><span class=cl>		<span class=n>target</span> <span class=o>=&gt;</span> <span class=s2>&#34;data&#34;</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span><span class=line><span class=cl>	<span class=n>mutate</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    	<span class=n>rename</span> <span class=o>=&gt;</span> <span class=p>{</span> <span class=s2>&#34;[data][level]&#34;</span> <span class=o>=&gt;</span> <span class=s2>&#34;level&#34;</span> <span class=p>}</span>
</span></span><span class=line><span class=cl>  	<span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>## Add your filters / logstash plugins configuration here</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=n>elasticsearch</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>		<span class=n>hosts</span> <span class=o>=&gt;</span> <span class=s2>&#34;elasticsearch:9200&#34;</span>
</span></span><span class=line><span class=cl>		<span class=n>user</span> <span class=o>=&gt;</span> <span class=s2>&#34;logstash_internal&#34;</span>
</span></span><span class=line><span class=cl>		<span class=n>password</span> <span class=o>=&gt;</span> <span class=s2>&#34;${LOGSTASH_INTERNAL_PASSWORD}&#34;</span>
</span></span><span class=line><span class=cl>		<span class=n>index</span> <span class=o>=&gt;</span> <span class=s2>&#34;logstash-%{+YYYY-MM-dd}&#34;</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><h2 id=特别鸣谢>特别鸣谢<a hidden class=anchor aria-hidden=true href=#特别鸣谢>#</a></h2><p>在本次配置 logstash 的过程中，我自己搭建的、使用 openAPI API 的、基于 GPT3.5 语言模型的 telegram bot 给予了我很大的帮助。</p><p>以下是我与它的对话(对话内容经过处理)</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mysql data-lang=mysql><span class=line><span class=cl><span class=n>johnpoint</span><span class=p>,</span><span class=w> </span><span class=p>[</span><span class=mi>5</span><span class=o>/</span><span class=mi>15</span><span class=o>/</span><span class=mi>23</span><span class=w> </span><span class=mi>2</span><span class=p>:</span><span class=mi>18</span><span class=err> </span><span class=n>PM</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>我想从</span><span class=w> </span><span class=mi>2023</span><span class=o>-</span><span class=mi>05</span><span class=o>-</span><span class=mi>15</span><span class=n>T13</span><span class=p>:</span><span class=mi>51</span><span class=p>:</span><span class=mi>00</span><span class=p>.</span><span class=mi>004687384</span><span class=o>+</span><span class=mi>08</span><span class=p>:</span><span class=mi>00</span><span class=w> </span><span class=n>stdout</span><span class=w> </span><span class=n>F</span><span class=w> </span><span class=err>{</span><span class=s2>&#34;level&#34;</span><span class=p>:</span><span class=s2>&#34;debug&#34;</span><span class=p>,</span><span class=s2>&#34;time&#34;</span><span class=p>:</span><span class=s2>&#34;2023-05-15T13:51:00.002+0800&#34;</span><span class=p>,</span><span class=s2>&#34;msg&#34;</span><span class=p>:</span><span class=s2>&#34;xxxxxxxx&#34;</span><span class=err>}</span><span class=w> </span><span class=err>这些数据里面使用</span><span class=n>logstash</span><span class=w> </span><span class=err>提取出</span><span class=n>json</span><span class=err>数据，</span><span class=n>logstash</span><span class=w> </span><span class=err>的</span><span class=n>pipeline</span><span class=err>该怎么写</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>openAI</span><span class=p>,</span><span class=w> </span><span class=p>[</span><span class=mi>5</span><span class=o>/</span><span class=mi>15</span><span class=o>/</span><span class=mi>23</span><span class=w> </span><span class=mi>2</span><span class=p>:</span><span class=mi>18</span><span class=err> </span><span class=n>PM</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>假设你的日志文件是以</span><span class=n>JSON</span><span class=err>格式输出的，你需要配置一个</span><span class=n>JSON</span><span class=err>解析器来解析日志中的</span><span class=n>JSON</span><span class=err>数据，并将其转换为可用的事件。以下是一个简单的</span><span class=n>Logstash</span><span class=err>配置文件：</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=o>```</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>input</span><span class=w> </span><span class=err>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=n>file</span><span class=w> </span><span class=err>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=n>path</span><span class=w> </span><span class=o>=&gt;</span><span class=w> </span><span class=s2>&#34;/path/to/your/logfile&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=n>start_position</span><span class=w> </span><span class=o>=&gt;</span><span class=w> </span><span class=s2>&#34;beginning&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=n>sincedb_path</span><span class=w> </span><span class=o>=&gt;</span><span class=w> </span><span class=s2>&#34;/dev/null&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>filter</span><span class=w> </span><span class=err>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=n>json</span><span class=w> </span><span class=err>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=n>source</span><span class=w> </span><span class=o>=&gt;</span><span class=w> </span><span class=s2>&#34;message&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>output</span><span class=w> </span><span class=err>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=n>stdout</span><span class=w> </span><span class=err>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=n>codec</span><span class=w> </span><span class=o>=&gt;</span><span class=w> </span><span class=n>rubydebug</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=o>```</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>在</span><span class=n>logstash</span><span class=err>的</span><span class=n>input</span><span class=err>部分，我们指定读取的文件的路径，以及从文件的开始位置开始读入数据。</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>在</span><span class=n>filter</span><span class=err>部分，我们使用</span><span class=o>`</span><span class=n>json</span><span class=o>`</span><span class=err>插件从日志中提取</span><span class=n>json</span><span class=err>对象。</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>最后，我们在</span><span class=n>output</span><span class=err>部分使用</span><span class=o>`</span><span class=n>rubydebug</span><span class=o>`</span><span class=err>进行输出。</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>运行这个</span><span class=n>Logstash</span><span class=err>的配置命令：</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=o>```</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>bin</span><span class=o>/</span><span class=n>logstash</span><span class=w> </span><span class=o>-</span><span class=n>f</span><span class=w> </span><span class=o>/</span><span class=n>path</span><span class=o>/</span><span class=k>to</span><span class=o>/</span><span class=n>your</span><span class=o>/</span><span class=n>config</span><span class=o>/</span><span class=n>file</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=o>```</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>这样，</span><span class=n>logstash</span><span class=err>就会从你指定的文件中读取数据，并把提取的</span><span class=n>JSON</span><span class=err>对象输出到控制台上。</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>johnpoint</span><span class=p>,</span><span class=w> </span><span class=p>[</span><span class=mi>5</span><span class=o>/</span><span class=mi>15</span><span class=o>/</span><span class=mi>23</span><span class=w> </span><span class=mi>2</span><span class=p>:</span><span class=mi>19</span><span class=err> </span><span class=n>PM</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>这个数据有</span><span class=n>json</span><span class=err>格式的数据，但是在</span><span class=n>json</span><span class=err>格式的数据前面还有一些脏数据，是否可以去除？</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>openAI</span><span class=p>,</span><span class=w> </span><span class=p>[</span><span class=mi>5</span><span class=o>/</span><span class=mi>15</span><span class=o>/</span><span class=mi>23</span><span class=w> </span><span class=mi>2</span><span class=p>:</span><span class=mi>19</span><span class=err> </span><span class=n>PM</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>可以，在使用</span><span class=w> </span><span class=n>logstash</span><span class=w> </span><span class=err>分析数据的时候，可以使用</span><span class=w> </span><span class=n>grok</span><span class=w> </span><span class=err>这个插件，通过正则表达式匹配出需要的数据。</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>以下是一个可能符合你提供的数据的</span><span class=w> </span><span class=n>grok</span><span class=w> </span><span class=err>表达式：</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=o>```</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=o>%</span><span class=err>{</span><span class=n>TIMESTAMP_ISO8601</span><span class=p>:</span><span class=kt>timestamp</span><span class=err>}</span><span class=w> </span><span class=n>stdout</span><span class=w> </span><span class=n>F</span><span class=w> </span><span class=o>%</span><span class=err>{</span><span class=n>GREEDYDATA</span><span class=p>:</span><span class=n>json_data</span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=o>```</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>这个正则表达式的含义是匹配以</span><span class=w> </span><span class=n>ISO8601</span><span class=w> </span><span class=err>格式表示的时间戳（如</span><span class=w> </span><span class=s2>&#34;2023-05-15T13:51:00.004687384+08:00&#34;</span><span class=err>）和以字符串</span><span class=w> </span><span class=s2>&#34;stdout F &#34;</span><span class=w> </span><span class=err>结尾的内容，然后跟着一个</span><span class=w> </span><span class=n>Json</span><span class=w> </span><span class=err>格式的字符串。</span><span class=s2>&#34;%{GREEDYDATA:json_data}&#34;</span><span class=w> </span><span class=err>表示将这个字符串保存到一个名为</span><span class=w> </span><span class=s2>&#34;json_data&#34;</span><span class=w> </span><span class=err>的字段中。</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>上述</span><span class=w> </span><span class=n>grok</span><span class=w> </span><span class=err>表达式可以在</span><span class=w> </span><span class=n>Logstash</span><span class=w> </span><span class=err>的配置文件中使用，例如：</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=o>```</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>input</span><span class=w> </span><span class=err>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=c1># 配置 logstash 接收数据的源
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>filter</span><span class=w> </span><span class=err>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=n>grok</span><span class=w> </span><span class=err>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>match</span><span class=w> </span><span class=o>=&gt;</span><span class=w> </span><span class=err>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=s2>&#34;message&#34;</span><span class=w> </span><span class=o>=&gt;</span><span class=w> </span><span class=s2>&#34;%{TIMESTAMP_ISO8601:timestamp} stdout F %{GREEDYDATA:json_data}&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=c1># 其他过滤器
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>output</span><span class=w> </span><span class=err>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=c1># 配置 logstash 输出数据的目标
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=o>```</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>这个配置文件中，通过</span><span class=w> </span><span class=n>filter</span><span class=w> </span><span class=err>下的</span><span class=w> </span><span class=n>grok</span><span class=w> </span><span class=err>插件匹配出了</span><span class=w> </span><span class=kt>timestamp</span><span class=w> </span><span class=err>和</span><span class=w> </span><span class=n>json_data</span><span class=w> </span><span class=err>两个字段。从而可以使用</span><span class=w> </span><span class=n>logstash</span><span class=w> </span><span class=err>利用这些数据进行更进一步的操作，如输出到</span><span class=w> </span><span class=n>Elasticsearch</span><span class=w> </span><span class=err>或者</span><span class=w> </span><span class=n>stdout</span><span class=err>。达到你想要的结果。</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>其中</span><span class=w> </span><span class=o>`%</span><span class=err>{</span><span class=n>TIMESTAMP_ISO8601</span><span class=p>:</span><span class=kt>timestamp</span><span class=err>}</span><span class=o>`</span><span class=w> </span><span class=err>表示匹配</span><span class=w> </span><span class=n>ISO8601</span><span class=w> </span><span class=err>格式的时间戳，并将其保存到一个名为</span><span class=w> </span><span class=s2>&#34;timestamp&#34;</span><span class=w> </span><span class=err>的字段中。</span><span class=o>`%</span><span class=err>{</span><span class=n>GREEDYDATA</span><span class=p>:</span><span class=n>json_data</span><span class=err>}</span><span class=o>`</span><span class=w> </span><span class=err>表示匹配任意字符，可以匹配行末附加的不符合</span><span class=w> </span><span class=n>Json</span><span class=w> </span><span class=err>格式的内容。因为</span><span class=w> </span><span class=n>grok</span><span class=w> </span><span class=err>会一直匹配到行尾，所以匹配过程中会包括最后一个字段（即</span><span class=w> </span><span class=n>json_data</span><span class=err>）中包含的所有内容。</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>johnpoint</span><span class=p>,</span><span class=w> </span><span class=p>[</span><span class=mi>5</span><span class=o>/</span><span class=mi>15</span><span class=o>/</span><span class=mi>23</span><span class=w> </span><span class=mi>2</span><span class=p>:</span><span class=mi>36</span><span class=err> </span><span class=n>PM</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>返回</span><span class=w> </span><span class=n>Can</span><span class=s1>&#39;t get text on a START_OBJECT at 1:8 是什么错误
</span></span></span><span class=line><span class=cl><span class=s1>
</span></span></span><span class=line><span class=cl><span class=s1>openAI, [5/15/23 2:36 PM]
</span></span></span><span class=line><span class=cl><span class=s1>这个错误常见于尝试将 JSON 对象作为字符串进行解析的情况下，但是实际上得到的是 JSON 对象，而不是字符串。这意味着该对象的开头是一个“{”，而不是一个“&#34;”。可以检查代码调用的函数或方法，以确保解析的字符串确实是一个字符串。另外，可以检查 JSON 对象的源代码，以确保它是按照预期格式进行编写的。
</span></span></span><span class=line><span class=cl><span class=s1>
</span></span></span><span class=line><span class=cl><span class=s1>johnpoint, [5/15/23 2:53 PM]
</span></span></span><span class=line><span class=cl><span class=s1>[2023-05-15T06:52:48,511][WARN ][logstash.filters.json    ][main][xxxxxx] Error parsing json {:source=&gt;&#34;json_data&#34;, :raw=&gt;&#34;{\\\&#34;level\\\&#34;:\\\&#34;debug\\\&#34;,\\\&#34;time\\\&#34;:\\\&#34;2023-05-15T14:52:47.664+0800\\\&#34;,\\\&#34;msg\\\&#34;:\\\&#34;xxxxxxxx\\\&#34;}\&#34;, \&#34;@timestamp\&#34;=&gt;0.1684133567674155e10}&#34;, :exception=&gt;#&lt;LogStash::Json::ParserError: Unexpected character (&#39;</span><span class=err>\</span><span class=s1>&#39; (code 92)): was expecting double-quote to start field name
</span></span></span><span class=line><span class=cl><span class=s1>这行日志表示 logstash 有什么问题呢
</span></span></span><span class=line><span class=cl><span class=s1>
</span></span></span><span class=line><span class=cl><span class=s1>openAI, [5/15/23 2:53 PM]
</span></span></span><span class=line><span class=cl><span class=s1>这行日志表示 Logstash 在解析输入数据 json_data 时遇到了编码问题，其中输入数据包含了 `\&#39;</span><span class=o>`</span><span class=w> </span><span class=err>等不符合</span><span class=w> </span><span class=n>JSON</span><span class=w> </span><span class=err>规范的字符，导致</span><span class=w> </span><span class=n>JSON</span><span class=w> </span><span class=err>解析器无法正确解析。具体来说，可能是输入数据中使用了反斜线对引号进行转义，而又没有正确地对反斜线本身进行转义。可以尝试对输入数据进行预处理，将不符合</span><span class=w> </span><span class=n>JSON</span><span class=w> </span><span class=err>规范的字符进行转义或删除，以便让</span><span class=w> </span><span class=n>Logstash</span><span class=w> </span><span class=err>能够正确解析数据。</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>johnpoint</span><span class=p>,</span><span class=w> </span><span class=p>[</span><span class=mi>5</span><span class=o>/</span><span class=mi>15</span><span class=o>/</span><span class=mi>23</span><span class=w> </span><span class=mi>2</span><span class=p>:</span><span class=mi>53</span><span class=err> </span><span class=n>PM</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>可以给出相对应</span><span class=w> </span><span class=err>的</span><span class=w> </span><span class=n>pipeline</span><span class=w> </span><span class=err>配置吗</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>openAI</span><span class=p>,</span><span class=w> </span><span class=p>[</span><span class=mi>5</span><span class=o>/</span><span class=mi>15</span><span class=o>/</span><span class=mi>23</span><span class=w> </span><span class=mi>2</span><span class=p>:</span><span class=mi>53</span><span class=err> </span><span class=n>PM</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>好的，以下是一个简单的</span><span class=w> </span><span class=n>Logstash</span><span class=w> </span><span class=n>Pipeline</span><span class=w> </span><span class=err>配置，用于从输入中提取</span><span class=w> </span><span class=o>`</span><span class=n>json_data</span><span class=o>`</span><span class=w> </span><span class=err>字段，并过滤掉任何无效数据：</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=o>```</span><span class=n>conf</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>input</span><span class=w> </span><span class=err>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=c1># 在这里配置输入插件
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=w>  </span><span class=n>stdin</span><span class=w> </span><span class=err>{}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>filter</span><span class=w> </span><span class=err>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=c1># 从 json_data 字段提取 JSON 数据
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=w>  </span><span class=n>json</span><span class=w> </span><span class=err>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=n>source</span><span class=w> </span><span class=o>=&gt;</span><span class=w> </span><span class=s2>&#34;json_data&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1># 用于忽略不完整的 JSON 数据字符串
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=w>    </span><span class=n>skip_on_invalid_json</span><span class=w> </span><span class=o>=&gt;</span><span class=w> </span><span class=no>true</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=c1># 过滤掉没有有效 JSON 数据的事件
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=w>  </span><span class=k>if</span><span class=w> </span><span class=s2>&#34;_jsonparsefailure&#34;</span><span class=w> </span><span class=k>in</span><span class=w> </span><span class=p>[</span><span class=n>tags</span><span class=p>]</span><span class=w> </span><span class=err>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>drop</span><span class=w> </span><span class=err>{}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>output</span><span class=w> </span><span class=err>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=c1># 在这里配置输出插件
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=w>  </span><span class=n>stdout</span><span class=w> </span><span class=err>{}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=o>```</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>您需要根据自己的需求进行相应的修改和调整，比如将</span><span class=w> </span><span class=o>`</span><span class=n>json_data</span><span class=o>`</span><span class=w> </span><span class=err>改成实际的字段名，或者使用适合您的输入和输出插件。</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>johnpoint</span><span class=p>,</span><span class=w> </span><span class=p>[</span><span class=mi>5</span><span class=o>/</span><span class=mi>15</span><span class=o>/</span><span class=mi>23</span><span class=w> </span><span class=mi>2</span><span class=p>:</span><span class=mi>54</span><span class=err> </span><span class=n>PM</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>使用</span><span class=n>grok</span><span class=err>提取的</span><span class=n>json</span><span class=err>数据会多出不必要的转义反斜杠</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>openAI</span><span class=p>,</span><span class=w> </span><span class=p>[</span><span class=mi>5</span><span class=o>/</span><span class=mi>15</span><span class=o>/</span><span class=mi>23</span><span class=w> </span><span class=mi>2</span><span class=p>:</span><span class=mi>54</span><span class=err> </span><span class=n>PM</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>是的，这是因为</span><span class=w> </span><span class=n>Grok</span><span class=w> </span><span class=err>模式匹配时会将反斜杠视为转义字符，因此如果日志中原本就有反斜杠，会被额外转义。为了解决这个问题，可以在配置文件中使用</span><span class=w> </span><span class=n>gsub</span><span class=w> </span><span class=err>过滤器去除多余的反斜杠。示例配置如下：</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=o>```</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>input</span><span class=w> </span><span class=err>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=n>stdin</span><span class=w> </span><span class=err>{</span><span class=w> </span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>filter</span><span class=w> </span><span class=err>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=n>grok</span><span class=w> </span><span class=err>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>match</span><span class=w> </span><span class=o>=&gt;</span><span class=w> </span><span class=err>{</span><span class=w> </span><span class=s2>&#34;message&#34;</span><span class=w> </span><span class=o>=&gt;</span><span class=w> </span><span class=s1>&#39;^\[%{TIMESTAMP_ISO8601:timestamp}\]\[%{WORD:log_level}\] %{GREEDYDATA:json_data}$&#39;</span><span class=w> </span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=n>mutate</span><span class=w> </span><span class=err>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=n>gsub</span><span class=w> </span><span class=o>=&gt;</span><span class=w> </span><span class=p>[</span><span class=s2>&#34;json_data&#34;</span><span class=p>,</span><span class=w> </span><span class=s2>&#34;\\\\&#34;</span><span class=p>,</span><span class=w> </span><span class=s2>&#34;\\&#34;</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=n>json</span><span class=w> </span><span class=err>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=n>source</span><span class=w> </span><span class=o>=&gt;</span><span class=w> </span><span class=s2>&#34;json_data&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>output</span><span class=w> </span><span class=err>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=n>stdout</span><span class=w> </span><span class=err>{</span><span class=w> </span><span class=n>codec</span><span class=w> </span><span class=o>=&gt;</span><span class=w> </span><span class=n>rubydebug</span><span class=w> </span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=o>```</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>这个配置中首先使用</span><span class=w> </span><span class=n>Grok</span><span class=w> </span><span class=err>解析出日志中的</span><span class=w> </span><span class=n>JSON</span><span class=w> </span><span class=err>数据，并将其存储到</span><span class=w> </span><span class=o>`</span><span class=n>json_data</span><span class=o>`</span><span class=w> </span><span class=err>字段中。然后使用</span><span class=w> </span><span class=o>`</span><span class=n>mutate</span><span class=o>`</span><span class=w> </span><span class=err>过滤器中的</span><span class=w> </span><span class=o>`</span><span class=n>gsub</span><span class=o>`</span><span class=w> </span><span class=err>将多余的反斜杠替换掉，最后使用</span><span class=w> </span><span class=o>`</span><span class=n>json</span><span class=o>`</span><span class=w> </span><span class=err>过滤器将其转换成</span><span class=w> </span><span class=n>JSON</span><span class=w> </span><span class=err>格式的事件。这样就可以避免多出不必要的反斜杠了。</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>johnpoint</span><span class=p>,</span><span class=w> </span><span class=p>[</span><span class=mi>5</span><span class=o>/</span><span class=mi>15</span><span class=o>/</span><span class=mi>23</span><span class=w> </span><span class=mi>3</span><span class=p>:</span><span class=mi>00</span><span class=err> </span><span class=n>PM</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>报错了</span><span class=w> </span><span class=n>Failed</span><span class=w> </span><span class=k>to</span><span class=w> </span><span class=n>execute</span><span class=w> </span><span class=n>action</span><span class=w> </span><span class=err>{</span><span class=p>:</span><span class=n>action</span><span class=o>=&gt;</span><span class=n>LogStash</span><span class=p>::</span><span class=n>PipelineAction</span><span class=p>::</span><span class=k>Create</span><span class=o>/</span><span class=n>pipeline_id</span><span class=p>:</span><span class=n>main</span><span class=p>,</span><span class=w> </span><span class=p>:</span><span class=n>exception</span><span class=o>=&gt;</span><span class=s2>&#34;LogStash::ConfigurationError&#34;</span><span class=p>,</span><span class=w> </span><span class=p>:</span><span class=n>message</span><span class=o>=&gt;</span><span class=s2>&#34;Expected one of [ \\t\\r\\n], \&#34;</span><span class=c1>#\&#34;, \&#34;{\&#34;, \&#34;,\&#34;, \&#34;]\&#34; at line 36, column 34 (byte 672) after filter {\n\tjson {\n\t\tsource =&gt; \&#34;message\&#34;\n\t\tremove_field =&gt; [ \n\t\t\t\&#34;message\&#34;\n\t\t]\n\t\ttarget =&gt; \&#34;kafkajson\&#34;\n\t}\n\tgrok {\n\t\tmatch =&gt; {\&#34;kafkajson\&#34; =&gt; \&#34;%{TIMESTAMP_ISO8601:timestamp} stdout F %{GREEDYDATA:json_data}\&#34;}\n\t}\n\tmutate {\n\t\tgsub =&gt; [\&#34;json_data\&#34;, \&#34;\\\\\\\\\&#34;, \&#34;&#34;, :backtrace=&gt;[&#34;/usr/share/logstash/logstash-core/lib/logstash/compiler.rb:32:in `compile_imperative&#39;&#34;, &#34;org/logstash/execution/AbstractPipelineExt.java:187:in `initialize&#39;&#34;, &#34;org/logstash/execution/JavaBasePipelineExt.java:72:in `initialize&#39;&#34;, &#34;/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:47:in `initialize&#39;&#34;, &#34;/usr/share/logstash/logstash-core/lib/logstash/pipeline_action/create.rb:52:in `execute&#39;&#34;, &#34;/usr/share/logstash/logstash-core/lib/logstash/agent.rb:383:in `block in converge_state&#39;&#34;]}
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>openAI</span><span class=p>,</span><span class=w> </span><span class=p>[</span><span class=mi>5</span><span class=o>/</span><span class=mi>15</span><span class=o>/</span><span class=mi>23</span><span class=w> </span><span class=mi>3</span><span class=p>:</span><span class=mi>00</span><span class=err> </span><span class=n>PM</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>这个错误是在</span><span class=w> </span><span class=n>Logstash</span><span class=w> </span><span class=err>配置文件中，第</span><span class=mi>36</span><span class=err>行，第</span><span class=mi>34</span><span class=err>列（</span><span class=mi>672</span><span class=err>个字节）处，期望看到</span><span class=w> </span><span class=o>`</span><span class=p>[</span><span class=w> </span><span class=err>\</span><span class=n>t</span><span class=err>\</span><span class=n>r</span><span class=err>\</span><span class=n>n</span><span class=p>]</span><span class=s2>&#34;, &#34;</span><span class=c1>#&#34;, &#34;{&#34;, &#34;,&#34;, &#34;]&#34;` 中的一个字符，但是实际遇到的字符无法按预期解析。这可能是因为语法错误或表达式不正确导致的。请检查您的配置文件并确保语法正确，并解决任何表达式错误。
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>另外，由于错误信息无法提供足够的上下文，您还可能需要检查其他部分的配置文件以确定是否存在其他错误。</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>johnpoint</span><span class=p>,</span><span class=w> </span><span class=p>[</span><span class=mi>5</span><span class=o>/</span><span class=mi>15</span><span class=o>/</span><span class=mi>23</span><span class=w> </span><span class=mi>3</span><span class=p>:</span><span class=mi>15</span><span class=err> </span><span class=n>PM</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>logstash</span><span class=w> </span><span class=err>可以将一个字段的数据移动到另一个字段里面去吗</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>openAI</span><span class=p>,</span><span class=w> </span><span class=p>[</span><span class=mi>5</span><span class=o>/</span><span class=mi>15</span><span class=o>/</span><span class=mi>23</span><span class=w> </span><span class=mi>3</span><span class=p>:</span><span class=mi>15</span><span class=err> </span><span class=n>PM</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>可以的，可以使用</span><span class=n>logstash</span><span class=err>的</span><span class=n>filter</span><span class=err>插件中的</span><span class=n>mutate</span><span class=err>插件的</span><span class=n>rename</span><span class=err>选项来实现</span><span class=p>.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>例如，如果数据来源的字段名为</span><span class=o>`</span><span class=n>source_field</span><span class=o>`</span><span class=err>，想要将其复制到目标字段</span><span class=o>`</span><span class=n>target_field</span><span class=o>`</span><span class=err>，可以使用以下配置</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=o>```</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>filter</span><span class=w> </span><span class=err>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=n>mutate</span><span class=w> </span><span class=err>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>rename</span><span class=w> </span><span class=o>=&gt;</span><span class=w> </span><span class=err>{</span><span class=w> </span><span class=s2>&#34;source_field&#34;</span><span class=w> </span><span class=o>=&gt;</span><span class=w> </span><span class=s2>&#34;target_field&#34;</span><span class=w> </span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=o>```</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>这将把</span><span class=o>`</span><span class=n>source_field</span><span class=o>`</span><span class=err>的值复制到新字段</span><span class=o>`</span><span class=n>target_field</span><span class=o>`</span><span class=err>中，并删除</span><span class=o>`</span><span class=n>source_field</span><span class=o>`</span><span class=err>字段本身。</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>johnpoint</span><span class=p>,</span><span class=w> </span><span class=p>[</span><span class=mi>5</span><span class=o>/</span><span class=mi>15</span><span class=o>/</span><span class=mi>23</span><span class=w> </span><span class=mi>3</span><span class=p>:</span><span class=mi>19</span><span class=err> </span><span class=n>PM</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>如果我要保留这个字段呢</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>openAI</span><span class=p>,</span><span class=w> </span><span class=p>[</span><span class=mi>5</span><span class=o>/</span><span class=mi>15</span><span class=o>/</span><span class=mi>23</span><span class=w> </span><span class=mi>3</span><span class=p>:</span><span class=mi>19</span><span class=err> </span><span class=n>PM</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>可以使用</span><span class=w> </span><span class=n>mutate</span><span class=w> </span><span class=err>插件的</span><span class=w> </span><span class=n>add_field</span><span class=w> </span><span class=err>选项，用于在事件中创建新字段。</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>比如，如果您想要在</span><span class=w> </span><span class=n>Logstash</span><span class=w> </span><span class=err>事件中创建一个新字段</span><span class=w> </span><span class=s2>&#34;new_field&#34;</span><span class=w> </span><span class=err>并从原始字段</span><span class=w> </span><span class=s2>&#34;old_field&#34;</span><span class=w> </span><span class=err>中移动其值：</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=o>```</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>filter</span><span class=w> </span><span class=err>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>   </span><span class=n>mutate</span><span class=w> </span><span class=err>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>     </span><span class=n>add_field</span><span class=w> </span><span class=o>=&gt;</span><span class=w> </span><span class=err>{</span><span class=w> </span><span class=s2>&#34;new_field&#34;</span><span class=w> </span><span class=o>=&gt;</span><span class=w> </span><span class=s2>&#34;%{[old_field]}&#34;</span><span class=w> </span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>     </span><span class=n>remove_field</span><span class=w> </span><span class=o>=&gt;</span><span class=w> </span><span class=p>[</span><span class=w> </span><span class=s2>&#34;old_field&#34;</span><span class=w> </span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>   </span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=o>```</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>这将将</span><span class=w> </span><span class=s2>&#34;old_field&#34;</span><span class=w> </span><span class=err>中的值赋给新的</span><span class=w> </span><span class=s2>&#34;new_field&#34;</span><span class=w> </span><span class=err>字段，并将删除</span><span class=w> </span><span class=s2>&#34;old_field&#34;</span><span class=w> </span><span class=err>字段。</span><span class=w> </span><span class=err>如果您想在保留源字段的同时移动值，可以将</span><span class=w> </span><span class=s2>&#34;remove_field&#34;</span><span class=w> </span><span class=err>选项删除。</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>johnpoint</span><span class=p>,</span><span class=w> </span><span class=p>[</span><span class=mi>5</span><span class=o>/</span><span class=mi>15</span><span class=o>/</span><span class=mi>23</span><span class=w> </span><span class=mi>3</span><span class=p>:</span><span class=mi>21</span><span class=err> </span><span class=n>PM</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>logstash</span><span class=w> </span><span class=err>删除自动创建</span><span class=w> </span><span class=n>keyword</span><span class=w> </span><span class=err>字段</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>openAI</span><span class=p>,</span><span class=w> </span><span class=p>[</span><span class=mi>5</span><span class=o>/</span><span class=mi>15</span><span class=o>/</span><span class=mi>23</span><span class=w> </span><span class=mi>3</span><span class=p>:</span><span class=mi>21</span><span class=err> </span><span class=n>PM</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>Logstash</span><span class=w> </span><span class=err>中默认会为每个字符串类型的字段创建一个</span><span class=w> </span><span class=o>`</span><span class=n>keyword</span><span class=o>`</span><span class=w> </span><span class=err>类型的子字段，作为原始字段的精确值。如果你不需要这个子字段，可以在索引模板（</span><span class=n>template</span><span class=err>）中将它移除：在</span><span class=w> </span><span class=o>`</span><span class=n>mappings</span><span class=o>`</span><span class=w> </span><span class=err>中设置</span><span class=w> </span><span class=o>`</span><span class=s2>&#34;include_in_all&#34;</span><span class=p>:</span><span class=w> </span><span class=no>false</span><span class=o>`</span><span class=w> </span><span class=err>即可。但需要注意的是，使用该方法删除</span><span class=w> </span><span class=o>`</span><span class=n>keyword</span><span class=o>`</span><span class=w> </span><span class=err>字段会导致失去部分精确查询的能力。</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>如果你想要在日志流中保留</span><span class=w> </span><span class=o>`</span><span class=n>keyword</span><span class=o>`</span><span class=w> </span><span class=err>字段，可以使用</span><span class=w> </span><span class=o>`</span><span class=n>remove_field</span><span class=o>`</span><span class=w> </span><span class=err>过滤器来删除原始字段，但保留</span><span class=w> </span><span class=o>`</span><span class=n>keyword</span><span class=o>`</span><span class=w> </span><span class=err>字段：</span><span class=w> 
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=o>```</span><span class=n>ruby</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>filter</span><span class=w> </span><span class=err>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=n>mutate</span><span class=w> </span><span class=err>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=n>remove_field</span><span class=w> </span><span class=o>=&gt;</span><span class=w> </span><span class=p>[</span><span class=s2>&#34;my_field&#34;</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>rename</span><span class=w> </span><span class=o>=&gt;</span><span class=w> </span><span class=err>{</span><span class=w> </span><span class=s2>&#34;my_field.keyword&#34;</span><span class=w> </span><span class=o>=&gt;</span><span class=w> </span><span class=s2>&#34;new_field&#34;</span><span class=w> </span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=o>```</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>上面的例子中，</span><span class=n>Logstash</span><span class=w> </span><span class=err>会删除原始的</span><span class=w> </span><span class=o>`</span><span class=n>my_field</span><span class=o>`</span><span class=w> </span><span class=err>字段，并将精确值保存在</span><span class=w> </span><span class=o>`</span><span class=n>my_field</span><span class=p>.</span><span class=n>keyword</span><span class=o>`</span><span class=w> </span><span class=err>子字段中，最后使用</span><span class=w> </span><span class=o>`</span><span class=k>rename</span><span class=o>`</span><span class=w> </span><span class=err>过滤器，将</span><span class=w> </span><span class=o>`</span><span class=n>my_field</span><span class=p>.</span><span class=n>keyword</span><span class=o>`</span><span class=w> </span><span class=err>重命名为</span><span class=w> </span><span class=o>`</span><span class=n>new_field</span><span class=o>`</span><span class=err>。这样，你就可以在</span><span class=w> </span><span class=n>Elasticsearch</span><span class=w> </span><span class=err>中使用</span><span class=w> </span><span class=o>`</span><span class=n>new_field</span><span class=o>`</span><span class=w> </span><span class=err>进行精确查询。</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>johnpoint</span><span class=p>,</span><span class=w> </span><span class=p>[</span><span class=mi>5</span><span class=o>/</span><span class=mi>15</span><span class=o>/</span><span class=mi>23</span><span class=w> </span><span class=mi>4</span><span class=p>:</span><span class=mi>18</span><span class=err> </span><span class=n>PM</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>logstash</span><span class=w> </span><span class=err>为什么创建的</span><span class=w> </span><span class=n>es</span><span class=w> </span><span class=k>index</span><span class=w> </span><span class=err>没有每天滚动的名字</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>openAI</span><span class=p>,</span><span class=w> </span><span class=p>[</span><span class=mi>5</span><span class=o>/</span><span class=mi>15</span><span class=o>/</span><span class=mi>23</span><span class=w> </span><span class=mi>4</span><span class=p>:</span><span class=mi>18</span><span class=err> </span><span class=n>PM</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>logstash</span><span class=w> </span><span class=err>创建</span><span class=w> </span><span class=n>ES</span><span class=w> </span><span class=k>index</span><span class=w> </span><span class=err>默认采用的是</span><span class=w> </span><span class=n>Daily</span><span class=w> </span><span class=k>Index</span><span class=w> </span><span class=err>的模式，每天一个</span><span class=w> </span><span class=k>Index</span><span class=err>，并且附带日期的后缀。例如，一个名为</span><span class=w> </span><span class=o>`</span><span class=n>logstash</span><span class=o>`</span><span class=w> </span><span class=err>的索引将被转换成</span><span class=w> </span><span class=o>`</span><span class=n>logstash</span><span class=o>-</span><span class=n>yyyy</span><span class=p>.</span><span class=n>mm</span><span class=p>.</span><span class=n>dd</span><span class=o>`</span><span class=w> </span><span class=err>的名字，在写入数据时，会根据</span><span class=w> </span><span class=n>UTC</span><span class=w> </span><span class=err>时区来决定对应日期的具体值。如果没有滚动，一种可能是配置文件中未启用，还有一个可能是</span><span class=w> </span><span class=n>Elasticsearch</span><span class=w> </span><span class=err>集群配置文件设置了禁止创建新</span><span class=w> </span><span class=k>index</span><span class=err>，可以检查一下</span><span class=w> </span><span class=n>Elasticsearch</span><span class=w> </span><span class=err>的配置文件中的相关设置。</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>johnpoint</span><span class=p>,</span><span class=w> </span><span class=p>[</span><span class=mi>5</span><span class=o>/</span><span class=mi>15</span><span class=o>/</span><span class=mi>23</span><span class=w> </span><span class=mi>4</span><span class=p>:</span><span class=mi>19</span><span class=err> </span><span class=n>PM</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>logstash</span><span class=w> </span><span class=err>怎么开启</span><span class=w> </span><span class=n>Daily</span><span class=w> </span><span class=k>Index</span><span class=w> </span><span class=err>模式</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>openAI</span><span class=p>,</span><span class=w> </span><span class=p>[</span><span class=mi>5</span><span class=o>/</span><span class=mi>15</span><span class=o>/</span><span class=mi>23</span><span class=w> </span><span class=mi>4</span><span class=p>:</span><span class=mi>19</span><span class=err> </span><span class=n>PM</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>要开启</span><span class=w> </span><span class=n>Daily</span><span class=w> </span><span class=k>Index</span><span class=w> </span><span class=err>模式，需要修改</span><span class=w> </span><span class=n>Logstash</span><span class=w> </span><span class=err>的配置文件。在</span><span class=w> </span><span class=o>`</span><span class=n>output</span><span class=o>`</span><span class=w> </span><span class=err>配置中，可以使用</span><span class=w> </span><span class=o>`</span><span class=k>index</span><span class=o>`</span><span class=w> </span><span class=err>配置项来指定</span><span class=w> </span><span class=n>Elasticsearch</span><span class=w> </span><span class=err>索引的名称。为了让索引名称按日滚动，可使用日期格式占位符，例如：</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=o>```</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>output</span><span class=w> </span><span class=err>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=n>elasticsearch</span><span class=w> </span><span class=err>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=n>hosts</span><span class=w> </span><span class=o>=&gt;</span><span class=w> </span><span class=p>[</span><span class=s2>&#34;localhost:9200&#34;</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>index</span><span class=w> </span><span class=o>=&gt;</span><span class=w> </span><span class=s2>&#34;your-index-name-%{+YYYY.MM.dd}&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=o>```</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>在上述配置中，索引名称的格式为</span><span class=w> </span><span class=o>`</span><span class=n>your</span><span class=o>-</span><span class=k>index</span><span class=o>-</span><span class=n>name</span><span class=o>-</span><span class=n>YYYY</span><span class=p>.</span><span class=n>MM</span><span class=p>.</span><span class=n>dd</span><span class=o>`</span><span class=err>，其中</span><span class=w> </span><span class=o>`%</span><span class=err>{</span><span class=o>+</span><span class=n>YYYY</span><span class=p>.</span><span class=n>MM</span><span class=p>.</span><span class=n>dd</span><span class=err>}</span><span class=o>`</span><span class=w> </span><span class=err>是一个日期格式占位符，表示当前日期。这样配置后，每天会自动创建一个新的索引。</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=err>还需注意的是，为了启用</span><span class=w> </span><span class=kt>Date</span><span class=w> </span><span class=k>Index</span><span class=w> </span><span class=err>模式，需要在</span><span class=w> </span><span class=n>Elasticsearch</span><span class=w> </span><span class=err>的配置文件中将</span><span class=w> </span><span class=o>`</span><span class=n>action</span><span class=p>.</span><span class=n>auto_create_index</span><span class=o>`</span><span class=w> </span><span class=err>设置为</span><span class=w> </span><span class=o>`</span><span class=no>true</span><span class=o>`</span><span class=err>。</span><span class=w>
</span></span></span></code></pre></div><h2 id=参考文档>参考文档<a hidden class=anchor aria-hidden=true href=#参考文档>#</a></h2><ul><li><a href=https://github.com/deviantony/docker-elk>Github: deviantony/docker-elk</a></li><li><a href=https://www.elastic.co/guide/en/logstash/current/plugins-filters-json.html>JSON filter plugin</a></li><li><a href=https://www.elastic.co/guide/en/logstash/current/plugins-filters-mutate.html#plugins-filters-mutate>Mutate filter plugin</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://blog.lvcshu.com/tags/logstash/>logstash</a></li><li><a href=https://blog.lvcshu.com/tags/%E8%85%BE%E8%AE%AF%E4%BA%91/>腾讯云</a></li><li><a href=https://blog.lvcshu.com/tags/%E7%AC%94%E8%AE%B0/>笔记</a></li></ul><script>var pageInfo=`{"RawContent":"\\n## 前提\\n\\n好久没有给博客除草了，正好最近折腾了下 logstash，记录一下。\\n\\n为啥要用 logstash 呢，其实是因为在测试环境上面腾讯云 tke 的日志没有开启日志收集，所以在排查问题的时候会十分的痛苦，正好有空了就想着将日志抽出来放进 es 里面，方便以后排查问题，正好看到腾讯云的日志规则是允许将 pod 的 stdout 日志进行采集之后投递到 kafka 的，就小试了一下。\\n\\n## 部署 logstash\\n\\nlogstash 我选择使用 docker-compose 来进行快速的部署。\\n\\n以下是部署流程，参考自 [deviantony/docker-elk](https://github.com/deviantony/docker-elk) 项目\\n\\n### 创建目录\\n\`\`\`shell\\nmkdir logstash/config logstash/pipeline -p\\n\`\`\`\\n\\n### 创建环境变量\\n路径 \`.env\`\\n\`\`\`ini\\nELASTIC_VERSION=8.7.1\\nLOGSTASH_INTERNAL_PASSWORD='changeme'\\n\`\`\`\\n\\n### 创建 Dockerfile\\n路径 \`logstasg/Dockerfile\`\\n\`\`\`Dockerfile\\nARG ELASTIC_VERSION\\n\\n# https://www.docker.elastic.co/\\nFROM docker.elastic.co/logstash/logstash:\${ELASTIC_VERSION}\\n\`\`\`\\n\\n### 配置文件\\n路径 \`logstash/config/logstash.yml\`\\n\`\`\`yml\\n---\\n## Default Logstash configuration from Logstash base image.\\n## https://github.com/elastic/logstash/blob/main/docker/data/logstash/config/logstash-full.yml\\n#\\nhttp.host: 0.0.0.0\\n\\nnode.name: logstash\\n\`\`\`\\n\\n路径 \`logstash/pipeline/logstash.conf\`\\n\`\`\`ruby\\ninput {\\n\\tbeats {\\n\\t\\tport =\\u003e 5044\\n\\t}\\n\\n\\ttcp {\\n\\t\\tport =\\u003e 50000\\n\\t}\\n}\\n\\n## Add your filters / logstash plugins configuration here\\n\\noutput {\\n\\telasticsearch {\\n\\t\\thosts =\\u003e \\"elasticsearch:9200\\"\\n\\t\\tuser =\\u003e \\"logstash_internal\\"\\n\\t\\tpassword =\\u003e \\"\${LOGSTASH_INTERNAL_PASSWORD}\\"\\n\\t\\tindex =\\u003e \\"logstash-%{+YYYY-MM-dd}\\"\\n\\t}\\n}\\n\`\`\`\\n\\n\\n### 启动服务\\n\`\`\`yaml\\nversion: '3.7'\\n\\nservices:\\n  logstash:\\n    build:\\n      context: logstash/\\n      args:\\n        ELASTIC_VERSION: \${ELASTIC_VERSION}\\n    volumes:\\n      - ./logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml:ro,Z\\n      - ./logstash/pipeline:/usr/share/logstash/pipeline:ro,Z\\n    ports:\\n      - 5044:5044\\n      - 50000:50000/tcp\\n      - 50000:50000/udp\\n      - 9600:9600\\n    environment:\\n      LS_JAVA_OPTS: -Xms256m -Xmx256m\\n      LOGSTASH_INTERNAL_PASSWORD: \${LOGSTASH_INTERNAL_PASSWORD:-}\\n    depends_on:\\n      - elasticsearch\\n    restart: unless-stopped\\n\`\`\`\\n\\n## 配置 logstash pipeline\\n\\n### 配置 input\\n因为日志要从 kafka 读取，所以要在 input 块内声明一个新的数据源\\n\\n\`\`\`ruby\\ninput {\\n\\tbeats {\\n\\t\\tport =\\u003e 5044\\n\\t}\\n\\n\\ttcp {\\n\\t\\tport =\\u003e 50000\\n\\t}\\n\\n\\tkafka {\\n\\t\\tbootstrap_servers =\\u003e \\"kafka 地址\\"\\n\\t\\tclient_id =\\u003e \\"test_logstash\\"\\n\\t\\tgroup_id =\\u003e \\"test_logstash\\"\\n\\t\\tauto_offset_reset =\\u003e \\"latest\\" \\n\\t\\tconsumer_threads =\\u003e 1\\n\\t\\tdecorate_events =\\u003e true \\n\\t\\ttopics =\\u003e [\\"tencent-tke-log\\"]\\n\\t\\ttype =\\u003e \\"bhy\\"\\n\\t}\\n}\\n\`\`\`\\n\\n### 配置过滤器\\n由于腾讯云投递到kafka的数据不是可以直接用的，数据示例如下\\n\`\`\`json\\n{\\"@timestamp\\":1684203000.007603,\\"@filepath\\":\\"/var/log/tke-log-agent/log/stdout-containerd/xxxxxxx.log\\",\\"log\\":\\"2023-05-16T10:10:00.002817673+08:00 stdout F {\\\\\\"level\\\\\\":\\\\\\"debug\\\\\\",\\\\\\"time\\\\\\":\\\\\\"2023-05-16T10:10:00.002+0800\\\\\\",\\\\\\"msg\\\\\\":\\\\\\"xxxxxxxx\\\\\\"}\\",\\"kubernetes\\":{\\"pod_name\\":\\"xxxxx\\",\\"namespace_name\\":\\"default\\",\\"pod_id\\":\\"xxxxxx\\",\\"labels\\":{\\"k8s-app\\":\\"xxxxx\\",\\"pod-template-hash\\":\\"xxxxx\\",\\"qcloud-app\\":\\"xxxxxxxx\\"},\\"annotations\\":{\\"qcloud-redeploy-timestamp\\":\\"1681975158658\\",\\"tke.cloud.tencent.com/networks-status\\":\\"json\\"},\\"host\\":\\"xxxxx\\",\\"container_name\\":\\"xxxx\\",\\"docker_id\\":\\"xxxxx\\",\\"container_hash\\":\\"xxxxxx\\",\\"container_image\\":\\"xxxxxxxx\\"}}\\n\`\`\`\\n\\n这时候就要对数据进行预处理，因为有很多数据是不需要添加进入es里面的。使用了 logstags 的 filter 进行处理。\\n\\n#### 保存原始数据\\n这个过滤器添加了一个叫做 source 的字段，并且将 kafka 消息全部存储在里面\\n\`\`\`ruby\\n\\tmutate {\\n\\t\\tadd_field =\\u003e { \\"source\\" =\\u003e \\"%{[message]}\\" }\\n  \\t}\\n\`\`\`\\n\\n#### 解析 json\\n使用 json 插件，对 message 字段的 json 字符串进行序列化，并且将序列化后的数据存储在 kafkajson 字段内，同时删除 message 字段\\n\`\`\`ruby\\n\\tjson {\\n\\t\\tsource =\\u003e \\"message\\"\\n\\t\\tremove_field =\\u003e [ \\n\\t\\t\\t\\"message\\"\\n\\t\\t]\\n\\t\\ttarget =\\u003e \\"kafkajson\\"\\n\\t}\\n\`\`\`\\n\\n#### 提取日志元数据\\n将日志中的一些定位数据提取到外层\\n\`\`\`ruby\\n\\tmutate {\\n    \\trename =\\u003e { \\"[kafkajson][kubernetes][namespace_name]\\" =\\u003e \\"namespace\\" }\\n    \\trename =\\u003e { \\"[kafkajson][kubernetes][pod_name]\\" =\\u003e \\"podname\\" }\\n    \\trename =\\u003e { \\"[kafkajson][kubernetes][labels][k8s-app]\\" =\\u003e \\"k8s-app\\" }\\n    \\trename =\\u003e { \\"[kafkajson][kubernetes][container_image]\\" =\\u003e \\"container-image\\" }\\n  \\t}\\n\`\`\`\\n\\n#### 使用正则表达式处理数据\\n腾讯云给出的数据会在服务自己打印的日志前方添加时间以及 ”stdout F“，所以要使用正则表达式将我们不需要的东西做下替换\\n\`\`\`shell\\n2023-05-16T10:10:00.002817673+08:00 stdout F {\\\\\\"level\\\\\\":\\\\\\"debug\\\\\\",\\\\\\"time\\\\\\":\\\\\\"2023-05-16T10:10:00.002+0800\\\\\\",\\\\\\"msg\\\\\\":\\\\\\"xxxxx\\\\\\"}\\n\`\`\`\\n\\n\`\`\`ruby\\n\\tmutate {\\n\\t\\tgsub =\\u003e [ \\"[kafkajson][log]\\",\\".+stdout\\\\sF\\\\s\\",\\"\\" ]\\n\\t}\\n\`\`\`\\n\\n#### 提取服务打印的日志\\n这回终于可以将日志提取出来了\\n\`\`\`ruby\\n\\tjson {\\n\\t\\tsource =\\u003e \\"[kafkajson][log]\\"\\n\\t\\tremove_field =\\u003e [ \\n\\t\\t\\t\\"kafkajson\\"\\n\\t\\t]\\n\\t\\ttarget =\\u003e \\"data\\"\\n\\t}\\n\`\`\`\\n\\n#### 提取日志等级\\n最后一步，将日志的等级字段提取到外层，方便使用\\n\`\`\`ruby\\n\\tmutate {\\n    \\trename =\\u003e { \\"[data][level]\\" =\\u003e \\"level\\" }\\n  \\t}\\n\`\`\`\\n\\n### 完整 pipeline\\n\`\`\`ruby\\ninput {\\n\\tbeats {\\n\\t\\tport =\\u003e 5044\\n\\t}\\n\\n\\ttcp {\\n\\t\\tport =\\u003e 50000\\n\\t}\\n\\n\\tkafka{\\n\\t\\tbootstrap_servers =\\u003e \\"kafka 地址\\"\\n\\t\\tclient_id =\\u003e \\"test_logstash\\"\\n\\t\\tgroup_id =\\u003e \\"test_logstash\\"\\n\\t\\tauto_offset_reset =\\u003e \\"latest\\" \\n\\t\\tconsumer_threads =\\u003e 1\\n\\t\\tdecorate_events =\\u003e true \\n\\t\\ttopics =\\u003e [\\"tencent-tke-log\\"]\\n\\t\\ttype =\\u003e \\"bhy\\"\\n\\t}\\n}\\n\\nfilter {\\n\\tmutate {\\n\\t\\tadd_field =\\u003e { \\"source\\" =\\u003e \\"%{[message]}\\" }\\n  \\t}\\n\\tjson {\\n\\t\\tsource =\\u003e \\"message\\"\\n\\t\\tremove_field =\\u003e [ \\n\\t\\t\\t\\"message\\"\\n\\t\\t]\\n\\t\\ttarget =\\u003e \\"kafkajson\\"\\n\\t}\\n\\tmutate {\\n    \\trename =\\u003e { \\"[kafkajson][kubernetes][namespace_name]\\" =\\u003e \\"namespace\\" }\\n    \\trename =\\u003e { \\"[kafkajson][kubernetes][pod_name]\\" =\\u003e \\"podname\\" }\\n    \\trename =\\u003e { \\"[kafkajson][kubernetes][labels][k8s-app]\\" =\\u003e \\"k8s-app\\" }\\n    \\trename =\\u003e { \\"[kafkajson][kubernetes][container_image]\\" =\\u003e \\"container-image\\" }\\n  \\t}\\n\\tmutate {\\n\\t\\tgsub =\\u003e [ \\"[kafkajson][log]\\",\\".+stdout\\\\sF\\\\s\\",\\"\\" ]\\n\\t}\\n\\tjson {\\n\\t\\tsource =\\u003e \\"[kafkajson][log]\\"\\n\\t\\tremove_field =\\u003e [ \\n\\t\\t\\t\\"kafkajson\\"\\n\\t\\t]\\n\\t\\ttarget =\\u003e \\"data\\"\\n\\t}\\n\\tmutate {\\n    \\trename =\\u003e { \\"[data][level]\\" =\\u003e \\"level\\" }\\n  \\t}\\n}\\n\\n## Add your filters / logstash plugins configuration here\\n\\noutput {\\n\\telasticsearch {\\n\\t\\thosts =\\u003e \\"elasticsearch:9200\\"\\n\\t\\tuser =\\u003e \\"logstash_internal\\"\\n\\t\\tpassword =\\u003e \\"\${LOGSTASH_INTERNAL_PASSWORD}\\"\\n\\t\\tindex =\\u003e \\"logstash-%{+YYYY-MM-dd}\\"\\n\\t}\\n}\\n\`\`\`\\n\\n## 特别鸣谢\\n\\n在本次配置 logstash 的过程中，我自己搭建的、使用 openAPI API 的、基于 GPT3.5 语言模型的 telegram bot 给予了我很大的帮助。\\n\\n以下是我与它的对话(对话内容经过处理)\\n\`\`\`\`\`\`\\njohnpoint, [5/15/23 2:18 PM]\\n我想从 2023-05-15T13:51:00.004687384+08:00 stdout F {\\"level\\":\\"debug\\",\\"time\\":\\"2023-05-15T13:51:00.002+0800\\",\\"msg\\":\\"xxxxxxxx\\"} 这些数据里面使用logstash 提取出json数据，logstash 的pipeline该怎么写\\n\\nopenAI, [5/15/23 2:18 PM]\\n假设你的日志文件是以JSON格式输出的，你需要配置一个JSON解析器来解析日志中的JSON数据，并将其转换为可用的事件。以下是一个简单的Logstash配置文件：\\n\\n\`\`\`\\ninput {\\n  file {\\n    path =\\u003e \\"/path/to/your/logfile\\"\\n    start_position =\\u003e \\"beginning\\"\\n    sincedb_path =\\u003e \\"/dev/null\\"\\n  }\\n}\\n\\nfilter {\\n  json {\\n    source =\\u003e \\"message\\"\\n  }\\n}\\n\\noutput {\\n  stdout {\\n      codec =\\u003e rubydebug\\n  }\\n}\\n\`\`\`\\n\\n在logstash的input部分，我们指定读取的文件的路径，以及从文件的开始位置开始读入数据。\\n在filter部分，我们使用\`json\`插件从日志中提取json对象。\\n最后，我们在output部分使用\`rubydebug\`进行输出。\\n\\n运行这个Logstash的配置命令：\\n\\n\`\`\`\\nbin/logstash -f /path/to/your/config/file\\n\`\`\`\\n\\n这样，logstash就会从你指定的文件中读取数据，并把提取的JSON对象输出到控制台上。\\n\\njohnpoint, [5/15/23 2:19 PM]\\n这个数据有json格式的数据，但是在json格式的数据前面还有一些脏数据，是否可以去除？\\n\\nopenAI, [5/15/23 2:19 PM]\\n可以，在使用 logstash 分析数据的时候，可以使用 grok 这个插件，通过正则表达式匹配出需要的数据。\\n\\n以下是一个可能符合你提供的数据的 grok 表达式：\\n\\n\`\`\`\\n%{TIMESTAMP_ISO8601:timestamp} stdout F %{GREEDYDATA:json_data}\\n\`\`\`\\n\\n这个正则表达式的含义是匹配以 ISO8601 格式表示的时间戳（如 \\"2023-05-15T13:51:00.004687384+08:00\\"）和以字符串 \\"stdout F \\" 结尾的内容，然后跟着一个 Json 格式的字符串。\\"%{GREEDYDATA:json_data}\\" 表示将这个字符串保存到一个名为 \\"json_data\\" 的字段中。\\n\\n上述 grok 表达式可以在 Logstash 的配置文件中使用，例如：\\n\\n\`\`\`\\ninput {\\n  # 配置 logstash 接收数据的源\\n}\\n\\nfilter {\\n  grok {\\n    match =\\u003e {\\n      \\"message\\" =\\u003e \\"%{TIMESTAMP_ISO8601:timestamp} stdout F %{GREEDYDATA:json_data}\\"\\n    }\\n  }\\n  # 其他过滤器\\n}\\n\\noutput {\\n  # 配置 logstash 输出数据的目标\\n}\\n\`\`\`\\n\\n这个配置文件中，通过 filter 下的 grok 插件匹配出了 timestamp 和 json_data 两个字段。从而可以使用 logstash 利用这些数据进行更进一步的操作，如输出到 Elasticsearch 或者 stdout。达到你想要的结果。\\n\\n其中 \`%{TIMESTAMP_ISO8601:timestamp}\` 表示匹配 ISO8601 格式的时间戳，并将其保存到一个名为 \\"timestamp\\" 的字段中。\`%{GREEDYDATA:json_data}\` 表示匹配任意字符，可以匹配行末附加的不符合 Json 格式的内容。因为 grok 会一直匹配到行尾，所以匹配过程中会包括最后一个字段（即 json_data）中包含的所有内容。\\n\\njohnpoint, [5/15/23 2:36 PM]\\n返回 Can't get text on a START_OBJECT at 1:8 是什么错误\\n\\nopenAI, [5/15/23 2:36 PM]\\n这个错误常见于尝试将 JSON 对象作为字符串进行解析的情况下，但是实际上得到的是 JSON 对象，而不是字符串。这意味着该对象的开头是一个“{”，而不是一个“\\"”。可以检查代码调用的函数或方法，以确保解析的字符串确实是一个字符串。另外，可以检查 JSON 对象的源代码，以确保它是按照预期格式进行编写的。\\n\\njohnpoint, [5/15/23 2:53 PM]\\n[2023-05-15T06:52:48,511][WARN ][logstash.filters.json    ][main][xxxxxx] Error parsing json {:source=\\u003e\\"json_data\\", :raw=\\u003e\\"{\\\\\\\\\\\\\\"level\\\\\\\\\\\\\\":\\\\\\\\\\\\\\"debug\\\\\\\\\\\\\\",\\\\\\\\\\\\\\"time\\\\\\\\\\\\\\":\\\\\\\\\\\\\\"2023-05-15T14:52:47.664+0800\\\\\\\\\\\\\\",\\\\\\\\\\\\\\"msg\\\\\\\\\\\\\\":\\\\\\\\\\\\\\"xxxxxxxx\\\\\\\\\\\\\\"}\\\\\\", \\\\\\"@timestamp\\\\\\"=\\u003e0.1684133567674155e10}\\", :exception=\\u003e#\\u003cLogStash::Json::ParserError: Unexpected character ('\\\\' (code 92)): was expecting double-quote to start field name\\n这行日志表示 logstash 有什么问题呢\\n\\nopenAI, [5/15/23 2:53 PM]\\n这行日志表示 Logstash 在解析输入数据 json_data 时遇到了编码问题，其中输入数据包含了 \`\\\\'\` 等不符合 JSON 规范的字符，导致 JSON 解析器无法正确解析。具体来说，可能是输入数据中使用了反斜线对引号进行转义，而又没有正确地对反斜线本身进行转义。可以尝试对输入数据进行预处理，将不符合 JSON 规范的字符进行转义或删除，以便让 Logstash 能够正确解析数据。\\n\\njohnpoint, [5/15/23 2:53 PM]\\n可以给出相对应 的 pipeline 配置吗\\n\\nopenAI, [5/15/23 2:53 PM]\\n好的，以下是一个简单的 Logstash Pipeline 配置，用于从输入中提取 \`json_data\` 字段，并过滤掉任何无效数据：\\n\\n\`\`\`conf\\ninput {\\n  # 在这里配置输入插件\\n  stdin {}\\n}\\n\\nfilter {\\n  # 从 json_data 字段提取 JSON 数据\\n  json {\\n    source =\\u003e \\"json_data\\"\\n    # 用于忽略不完整的 JSON 数据字符串\\n    skip_on_invalid_json =\\u003e true\\n  }\\n\\n  # 过滤掉没有有效 JSON 数据的事件\\n  if \\"_jsonparsefailure\\" in [tags] {\\n    drop {}\\n  }\\n}\\n\\noutput {\\n  # 在这里配置输出插件\\n  stdout {}\\n}\\n\`\`\`\\n\\n您需要根据自己的需求进行相应的修改和调整，比如将 \`json_data\` 改成实际的字段名，或者使用适合您的输入和输出插件。\\n\\njohnpoint, [5/15/23 2:54 PM]\\n使用grok提取的json数据会多出不必要的转义反斜杠\\n\\nopenAI, [5/15/23 2:54 PM]\\n是的，这是因为 Grok 模式匹配时会将反斜杠视为转义字符，因此如果日志中原本就有反斜杠，会被额外转义。为了解决这个问题，可以在配置文件中使用 gsub 过滤器去除多余的反斜杠。示例配置如下：\\n\\n\`\`\`\\ninput {\\n  stdin { }\\n}\\n\\nfilter {\\n  grok {\\n    match =\\u003e { \\"message\\" =\\u003e '^\\\\[%{TIMESTAMP_ISO8601:timestamp}\\\\]\\\\[%{WORD:log_level}\\\\] %{GREEDYDATA:json_data}$' }\\n  }\\n  mutate {\\n    gsub =\\u003e [\\"json_data\\", \\"\\\\\\\\\\\\\\\\\\", \\"\\\\\\\\\\"]\\n  }\\n  json {\\n    source =\\u003e \\"json_data\\"\\n  }\\n}\\n\\noutput {\\n  stdout { codec =\\u003e rubydebug }\\n}\\n\`\`\`\\n\\n这个配置中首先使用 Grok 解析出日志中的 JSON 数据，并将其存储到 \`json_data\` 字段中。然后使用 \`mutate\` 过滤器中的 \`gsub\` 将多余的反斜杠替换掉，最后使用 \`json\` 过滤器将其转换成 JSON 格式的事件。这样就可以避免多出不必要的反斜杠了。\\n\\njohnpoint, [5/15/23 3:00 PM]\\n报错了 Failed to execute action {:action=\\u003eLogStash::PipelineAction::Create/pipeline_id:main, :exception=\\u003e\\"LogStash::ConfigurationError\\", :message=\\u003e\\"Expected one of [ \\\\\\\\t\\\\\\\\r\\\\\\\\n], \\\\\\"#\\\\\\", \\\\\\"{\\\\\\", \\\\\\",\\\\\\", \\\\\\"]\\\\\\" at line 36, column 34 (byte 672) after filter {\\\\n\\\\tjson {\\\\n\\\\t\\\\tsource =\\u003e \\\\\\"message\\\\\\"\\\\n\\\\t\\\\tremove_field =\\u003e [ \\\\n\\\\t\\\\t\\\\t\\\\\\"message\\\\\\"\\\\n\\\\t\\\\t]\\\\n\\\\t\\\\ttarget =\\u003e \\\\\\"kafkajson\\\\\\"\\\\n\\\\t}\\\\n\\\\tgrok {\\\\n\\\\t\\\\tmatch =\\u003e {\\\\\\"kafkajson\\\\\\" =\\u003e \\\\\\"%{TIMESTAMP_ISO8601:timestamp} stdout F %{GREEDYDATA:json_data}\\\\\\"}\\\\n\\\\t}\\\\n\\\\tmutate {\\\\n\\\\t\\\\tgsub =\\u003e [\\\\\\"json_data\\\\\\", \\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\", \\\\\\"\\", :backtrace=\\u003e[\\"/usr/share/logstash/logstash-core/lib/logstash/compiler.rb:32:in \`compile_imperative'\\", \\"org/logstash/execution/AbstractPipelineExt.java:187:in \`initialize'\\", \\"org/logstash/execution/JavaBasePipelineExt.java:72:in \`initialize'\\", \\"/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:47:in \`initialize'\\", \\"/usr/share/logstash/logstash-core/lib/logstash/pipeline_action/create.rb:52:in \`execute'\\", \\"/usr/share/logstash/logstash-core/lib/logstash/agent.rb:383:in \`block in converge_state'\\"]}\\n\\nopenAI, [5/15/23 3:00 PM]\\n这个错误是在 Logstash 配置文件中，第36行，第34列（672个字节）处，期望看到 \`[ \\\\t\\\\r\\\\n]\\", \\"#\\", \\"{\\", \\",\\", \\"]\\"\` 中的一个字符，但是实际遇到的字符无法按预期解析。这可能是因为语法错误或表达式不正确导致的。请检查您的配置文件并确保语法正确，并解决任何表达式错误。\\n\\n另外，由于错误信息无法提供足够的上下文，您还可能需要检查其他部分的配置文件以确定是否存在其他错误。\\n\\njohnpoint, [5/15/23 3:15 PM]\\nlogstash 可以将一个字段的数据移动到另一个字段里面去吗\\n\\nopenAI, [5/15/23 3:15 PM]\\n可以的，可以使用logstash的filter插件中的mutate插件的rename选项来实现.\\n\\n例如，如果数据来源的字段名为\`source_field\`，想要将其复制到目标字段\`target_field\`，可以使用以下配置:\\n\\n\`\`\`\\nfilter {\\n  mutate {\\n    rename =\\u003e { \\"source_field\\" =\\u003e \\"target_field\\" }\\n  }\\n}\\n\`\`\`\\n\\n这将把\`source_field\`的值复制到新字段\`target_field\`中，并删除\`source_field\`字段本身。\\n\\njohnpoint, [5/15/23 3:19 PM]\\n如果我要保留这个字段呢\\n\\nopenAI, [5/15/23 3:19 PM]\\n可以使用 mutate 插件的 add_field 选项，用于在事件中创建新字段。\\n\\n比如，如果您想要在 Logstash 事件中创建一个新字段 \\"new_field\\" 并从原始字段 \\"old_field\\" 中移动其值：\\n\\n\`\`\`\\nfilter {\\n   mutate {\\n     add_field =\\u003e { \\"new_field\\" =\\u003e \\"%{[old_field]}\\" }\\n     remove_field =\\u003e [ \\"old_field\\" ]\\n   }\\n}\\n\`\`\`\\n\\n这将将 \\"old_field\\" 中的值赋给新的 \\"new_field\\" 字段，并将删除 \\"old_field\\" 字段。 如果您想在保留源字段的同时移动值，可以将 \\"remove_field\\" 选项删除。\\n\\njohnpoint, [5/15/23 3:21 PM]\\nlogstash 删除自动创建 keyword 字段\\n\\nopenAI, [5/15/23 3:21 PM]\\nLogstash 中默认会为每个字符串类型的字段创建一个 \`keyword\` 类型的子字段，作为原始字段的精确值。如果你不需要这个子字段，可以在索引模板（template）中将它移除：在 \`mappings\` 中设置 \`\\"include_in_all\\": false\` 即可。但需要注意的是，使用该方法删除 \`keyword\` 字段会导致失去部分精确查询的能力。\\n\\n如果你想要在日志流中保留 \`keyword\` 字段，可以使用 \`remove_field\` 过滤器来删除原始字段，但保留 \`keyword\` 字段： \\n\\n\`\`\`ruby\\nfilter {\\n  mutate {\\n    remove_field =\\u003e [\\"my_field\\"]\\n    rename =\\u003e { \\"my_field.keyword\\" =\\u003e \\"new_field\\" }\\n  }\\n}\\n\`\`\`\\n\\n上面的例子中，Logstash 会删除原始的 \`my_field\` 字段，并将精确值保存在 \`my_field.keyword\` 子字段中，最后使用 \`rename\` 过滤器，将 \`my_field.keyword\` 重命名为 \`new_field\`。这样，你就可以在 Elasticsearch 中使用 \`new_field\` 进行精确查询。\\n\\njohnpoint, [5/15/23 4:18 PM]\\nlogstash 为什么创建的 es index 没有每天滚动的名字\\n\\nopenAI, [5/15/23 4:18 PM]\\nlogstash 创建 ES index 默认采用的是 Daily Index 的模式，每天一个 Index，并且附带日期的后缀。例如，一个名为 \`logstash\` 的索引将被转换成 \`logstash-yyyy.mm.dd\` 的名字，在写入数据时，会根据 UTC 时区来决定对应日期的具体值。如果没有滚动，一种可能是配置文件中未启用，还有一个可能是 Elasticsearch 集群配置文件设置了禁止创建新 index，可以检查一下 Elasticsearch 的配置文件中的相关设置。\\n\\njohnpoint, [5/15/23 4:19 PM]\\nlogstash 怎么开启 Daily Index 模式\\n\\nopenAI, [5/15/23 4:19 PM]\\n要开启 Daily Index 模式，需要修改 Logstash 的配置文件。在 \`output\` 配置中，可以使用 \`index\` 配置项来指定 Elasticsearch 索引的名称。为了让索引名称按日滚动，可使用日期格式占位符，例如：\\n\\n\`\`\`\\noutput {\\n  elasticsearch {\\n    hosts =\\u003e [\\"localhost:9200\\"]\\n    index =\\u003e \\"your-index-name-%{+YYYY.MM.dd}\\"\\n  }\\n}\\n\`\`\`\\n\\n在上述配置中，索引名称的格式为 \`your-index-name-YYYY.MM.dd\`，其中 \`%{+YYYY.MM.dd}\` 是一个日期格式占位符，表示当前日期。这样配置后，每天会自动创建一个新的索引。\\n\\n还需注意的是，为了启用 Date Index 模式，需要在 Elasticsearch 的配置文件中将 \`action.auto_create_index\` 设置为 \`true\`。\\n\`\`\`\`\`\`\\n\\n## 参考文档\\n- [Github: deviantony/docker-elk](https://github.com/deviantony/docker-elk)\\n- [JSON filter plugin](https://www.elastic.co/guide/en/logstash/current/plugins-filters-json.html)\\n- [Mutate filter plugin](https://www.elastic.co/guide/en/logstash/current/plugins-filters-mutate.html#plugins-filters-mutate)","ResourceType":"page","MediaType":{"mainType":"application","subType":"octet-stream","delimiter":".","firstSuffix":{"suffix":"","fullSuffix":""},"type":"application/octet-stream","string":"application/octet-stream","suffixes":[""]},"Permalink":"https://blog.lvcshu.com/2023/05/15/%E4%BD%BF%E7%94%A8-logstash-%E9%87%87%E9%9B%86%E6%9D%A5%E8%87%AA%E8%85%BE%E8%AE%AF%E4%BA%91-tke-%E7%9A%84%E6%97%A5%E5%BF%97/","RelPermalink":"/2023/05/15/%E4%BD%BF%E7%94%A8-logstash-%E9%87%87%E9%9B%86%E6%9D%A5%E8%87%AA%E8%85%BE%E8%AE%AF%E4%BA%91-tke-%E7%9A%84%E6%97%A5%E5%BF%97/","Name":"使用 logstash 采集来自腾讯云 tke 的日志","Title":"使用 logstash 采集来自腾讯云 tke 的日志","Params":{"date":"2023-05-15T10:34:44Z","draft":false,"iscjklanguage":false,"lastmod":"2023-05-15T10:34:44Z","publishdate":"2023-05-15T10:34:44Z","slug":"使用 logstash 采集来自腾讯云 tke 的日志","tags":["logstash","腾讯云","笔记"],"title":"使用 logstash 采集来自腾讯云 tke 的日志","toc":true},"Data":{},"Date":"2023-05-15T10:34:44Z","Lastmod":"2023-05-15T10:34:44Z","PublishDate":"2023-05-15T10:34:44Z","ExpiryDate":"0001-01-01T00:00:00Z","Aliases":null,"BundleType":"","Description":"","Draft":false,"IsHome":false,"Keywords":null,"Kind":"page","Layout":"","LinkTitle":"使用 logstash 采集来自腾讯云 tke 的日志","IsNode":false,"IsPage":true,"Path":"posts/logstash-tencent-tke-log.md","Pathc":"posts/logstash-tencent-tke-log.md","Slug":"使用 logstash 采集来自腾讯云 tke 的日志","Lang":"en","IsSection":false,"Section":"posts","SectionsEntries":["posts"],"SectionsPath":"posts","Sitemap":{"ChangeFreq":"","Priority":-1,"Filename":"sitemap.xml"},"Type":"posts","Weight":0,"Language":{"Lang":"en","LanguageName":"","LanguageDirection":"","Title":"","Weight":0,"Disabled":false,"ContentDir":"","Cfg":{},"LocalCfg":{},"Provider":{}},"File":{"File":{}},"GitInfo":{"hash":"","abbreviatedHash":"","subject":"","authorName":"","authorEmail":"","authorDate":"0001-01-01T00:00:00Z","commitDate":"0001-01-01T00:00:00Z"},"CodeOwners":null,"OutputFormats":[{"Rel":"canonical","Format":{"mediaType":"text/html","name":"HTML","path":"","baseName":"index","rel":"canonical","protocol":"","isPlainText":false,"isHTML":true,"noUgly":false,"notAlternative":false,"permalinkable":true,"weight":10}}],"AlternativeOutputFormats":null,"Menus":null,"TranslationKey":"page/posts/logstash-tencent-tke-log","IsTranslated":false,"AllTranslations":null,"Translations":null,"Store":{},"GetIdentity":{"Type":"content","Path":"posts/logstash-tencent-tke-log.md"}}`</script><div><ul class=post-copyright><li class=post-copyright-author><a>本文作者: </a>johnpoint</li><li class=post-copyright-link><a>本文链接:</a>
<a href id=copyrightlink>https://blog.lvcshu.com/2023/05/15/%E4%BD%BF%E7%94%A8-logstash-%E9%87%87%E9%9B%86%E6%9D%A5%E8%87%AA%E8%85%BE%E8%AE%AF%E4%BA%91-tke-%E7%9A%84%E6%97%A5%E5%BF%97/</a></li><li class=post-copyright-license><a>版权声明:</a>
本作品采用<a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/>知识共享署名-相同方式共享 4.0 国际许可协议</a>进行许可。</li><svg xmlns="http://www.w3.org/2000/svg" class="copyright-img" viewBox="0 0 496 512"><path fill="#4a4a4a" d="m245.8 214.9-33.2 17.3c-9.4-19.6-25.2-20-27.4-20-22.2.0-33.3 14.6-33.3 43.9.0 23.5 9.2 43.8 33.3 43.8 14.4.0 24.6-7 30.5-21.3l30.6 15.5a73.2 73.2.0 01-65.1 39c-22.6.0-74-10.3-74-77 0-58.7 43-77 72.6-77 30.8-.1 52.7 11.9 66 35.8zm143 0-32.7 17.3c-9.5-19.8-25.7-20-27.9-20-22.1.0-33.2 14.6-33.2 43.9.0 23.5 9.2 43.8 33.2 43.8 14.5.0 24.7-7 30.5-21.3l31 15.5c-2 3.8-21.3 39-65 39-22.7.0-74-9.9-74-77 0-58.7 43-77 72.6-77C354 179 376 191 389 214.8zM247.7 8C104.7 8 0 123 0 256c0 138.4 113.6 248 247.6 248C377.5 504 496 403 496 256 496 118 389.4 8 247.6 8zm.8 450.8c-112.5.0-203.7-93-203.7-202.8.0-105.5 85.5-203.3 203.8-203.3A201.7 201.7.0 01451.3 256c0 121.7-99.7 202.9-202.9 202.9z"/></svg></ul></div><nav class=paginav><a class=next href=https://blog.lvcshu.com/2022/12/29/2022-year-summary/><span class=title>Next »</span><br><span>2022 年度总结</span></a></nav><div id=disqus_thread></div><script>(function(){var e=document,t=e.createElement("script");t.src="https://johnpoints-blog-hexo.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></footer></article><style>.copyright-img{position:absolute;width:128px;height:128px;top:-35px;right:5%;opacity:.1;filter:invert(100%)}.post-copyright{border-radius:var(--radius);margin:2em 0 0;padding:.5em 1em;border-left:3px solid #ff1700;background-color:var(--code-bg);list-style:none;position:relative;overflow:hidden}.post-copyright li{margin:8px 0}</style></main><footer class=footer><span>&copy; 2023 <a href=https://blog.lvcshu.com/>johnpoint's blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script async defer data-website-id=7319ce07-a672-4d99-bf07-9ebb227d449d src=https://umami.uipo.cc/umami.js></script></body></html>